{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxIVFFKtqNOGLg8usBRCNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caseyeaston/BEA_PipelineEngagementAnalysis/blob/main/BEA_PipelineEngagementCleaningFinalv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "n9Wy7FDaG6qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LKl1obNgsqeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53127965-e76d-4a63-a045-d8e547832b21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', None)"
      ],
      "metadata": {
        "id": "1njSdg8Pss0Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths\n",
        "base_path = '/content/drive/MyDrive/Work/BEA/2025 BEA Data Project Shared Folder/Data/(Main) Data Sources/Existing/PPBEA Pipeline/CSVs/'\n",
        "file_paths = {\n",
        "    '2019-2020': f'{base_path}2019-2020_PPBEA Pipeline_Engagement.csv',\n",
        "    '2020-2021': f'{base_path}2020-2021_PPBEA Pipeline_Engagement.csv',\n",
        "    '2021-2022': f'{base_path}2021-2022_PPBEA Pipeline_Engagement.csv',\n",
        "    '2022-2023': f'{base_path}2022-2023_PPBEA Pipeline_Engagement.csv',\n",
        "    '2023-2024': f'{base_path}2023-2024_PPBEA Pipeline_Engagement.csv',\n",
        "    '2024-2025': f'{base_path}2024-2025_PPBEA Pipeline_Engagement.csv',\n",
        "}\n",
        "\n",
        "# Load all CSV files\n",
        "dfs = {}\n",
        "for year, path in file_paths.items():\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.dropna(how='all')  # Remove completely empty rows\n",
        "    df['School Year'] = year  # Add school year identifier\n",
        "    dfs[year] = df\n",
        "\n",
        "# Combine all dataframes\n",
        "dfmain = pd.concat(dfs.values(), ignore_index=True)"
      ],
      "metadata": {
        "id": "qku6mOr_tMZK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename PPBEA Member to District\n",
        "dfmain = dfmain.rename(columns={'PPBEA Member': 'District'})"
      ],
      "metadata": {
        "id": "--cAFXytypfW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop district columns and other unwanted columns\n",
        "columns_to_drop = [\n",
        "    ' ',  # Unnamed column\n",
        "    'Calhan District RJ-1', 'Harrison District 2', 'Widefield District 3',\n",
        "    'Fountain Ft.Carson District 8', 'Colorado Springs District 11',\n",
        "    'Cheyenne Mountain District 12', 'Manitou Springs District 14',\n",
        "    'Academy District 20', 'Ellicott District 22', 'Peyton District 23JT',\n",
        "    'Lewis Palmer District 38', 'El Paso County District 49',\n",
        "    'Colorado Springs Early College (CSEC)', 'CO Digital BOCES PPOS & CPA',\n",
        "    'Eastlake High School', 'Banning Lewis Ranch', 'Atlas Prep',\n",
        "    'Woodland Park School District',\n",
        "    'Unnamed: 24',\n",
        "    'Career Rep Email', 'Follow-up Task: ', 'Employer post Internship',\n",
        "    'Sponsor Email', 'Placed into Employment Post Internship',\n",
        "    'Staff Interactions with Businesses', 'Career Rep First Name',\n",
        "    'Career Rep Last Name', 'Opp Number', 'Task Number',\n",
        "    'PPBEA Staff Assigned', 'Next Action', 'PPBEA Notes',\n",
        "    'Notes: Student Name, Duration, School Name, Sponsor Name, Teacher Name, Flags'\n",
        "]\n",
        "dfmain = dfmain.drop(columns=columns_to_drop)"
      ],
      "metadata": {
        "id": "cc03uaaDtOPE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge duplicate columns\n",
        "dfmain['Pro101 Certificates Earned'] = dfmain['Pro101 Certificates Earned'].fillna(\n",
        "    dfmain['Professionalism 101 Certificates Earned']\n",
        ")\n",
        "dfmain = dfmain.drop(columns=[\n",
        "    'Professionalism 101 Certificates Earned',\n",
        "])"
      ],
      "metadata": {
        "id": "S0IklkYRtQGj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numeric columns\n",
        "numeric_cols = [\n",
        "    'Complete Student Trainings',\n",
        "    'Complete Staff Trainings',\n",
        "    'Complete Student Interactions',\n",
        "    'Complete Student Internships',\n",
        "    'Internships in Progress',\n",
        "    'Pending Student Interactions',\n",
        "    'Declined or Cancelled Student Interactions',\n",
        "    'Pro101 Certificates Earned'\n",
        "]\n",
        "\n",
        "for col in numeric_cols:\n",
        "    dfmain[col] = pd.to_numeric(dfmain[col], errors='coerce')\n",
        "\n",
        "# Fill nulls with 0 for numeric columns\n",
        "for col in numeric_cols:\n",
        "    dfmain[col] = dfmain[col].fillna(0)"
      ],
      "metadata": {
        "id": "xMsl0xOhtSPr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove summary and empty rows\n",
        "dfmain = dfmain[\n",
        "    ((dfmain['Complete Student Interactions'] <= 2000) | (dfmain['Complete Student Interactions'].isna())) &\n",
        "    (dfmain['Event Title'].notna())\n",
        "]"
      ],
      "metadata": {
        "id": "F2sQ9xRStTVt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date columns to datetime\n",
        "dfmain['Initiation Date'] = pd.to_datetime(dfmain['Initiation Date'], errors='coerce')\n",
        "dfmain['Status Update Date'] = pd.to_datetime(dfmain['Status Update Date'], errors='coerce')\n",
        "dfmain['Event Date or Start Date'] = pd.to_datetime(dfmain['Event Date or Start Date'], errors='coerce')"
      ],
      "metadata": {
        "id": "ghqTAKvyu6nf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize WBL Opportunity Type\n",
        "dfmain['WBL Opportunity Type'] = dfmain['WBL Opportunity Type'].replace({\n",
        "    \"Speaker's Bureau\": \"Speakers Bureau\"\n",
        "})\n",
        "\n",
        "# Drop one-off WBL type\n",
        "dfmain = dfmain[dfmain['WBL Opportunity Type'] != 'Jobs/Training/Apprenticeship']\n",
        "\n",
        "# Clean text columns (strip whitespace/newlines)\n",
        "dfmain = dfmain.copy()\n",
        "dfmain['Business Champion Name'] = dfmain['Business Champion Name'].str.strip()\n",
        "dfmain = dfmain.rename(columns={'Student and Sponsor\\nor School POC Name': 'Student Sponsor Name'})\n",
        "dfmain['Student Sponsor Name'] = dfmain['Student Sponsor Name'].str.strip()"
      ],
      "metadata": {
        "id": "V9HD_f3ubckR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking shape\n",
        "dfmain.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHewuA1fod71",
        "outputId": "31295d6b-aed3-4ab5-8a2c-7e6d9eed07b8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10068, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Date Errors and Nulls"
      ],
      "metadata": {
        "id": "98vpbrd3IHDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix unrealistic dates (before 2018)\n",
        "date_cols = ['Initiation Date', 'Status Update Date', 'Event Date or Start Date']\n",
        "cutoff_date = pd.Timestamp('2018-01-01')\n",
        "\n",
        "for col in date_cols:\n",
        "    unrealistic = dfmain[col] < cutoff_date\n",
        "    count = unrealistic.sum()\n",
        "    if count > 0:\n",
        "        print(f\"Setting {count} unrealistic dates to null in {col}\")\n",
        "        dfmain.loc[unrealistic, col] = pd.NaT\n",
        "\n",
        "# Fill remaining nulls in date columns using cascade logic\n",
        "dfmain['Initiation Date'] = dfmain['Initiation Date'].fillna(dfmain['Event Date or Start Date']).fillna(dfmain['Status Update Date'])\n",
        "dfmain['Status Update Date'] = dfmain['Status Update Date'].fillna(dfmain['Event Date or Start Date']).fillna(dfmain['Initiation Date'])\n",
        "\n",
        "# Create Derived Event Date column with fallback logic\n",
        "# Priority: Event Date or Start Date → Status Update Date → Initiation Date\n",
        "dfmain['Derived Event Date'] = dfmain['Event Date or Start Date'].fillna(\n",
        "    dfmain['Status Update Date']\n",
        ").fillna(\n",
        "    dfmain['Initiation Date']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASOzjm7_QCW3",
        "outputId": "d27ad3b7-d348-420b-cfc0-295e4f66cc8c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting 4 unrealistic dates to null in Initiation Date\n",
            "Setting 7 unrealistic dates to null in Status Update Date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 'Placement Status' Parent Column & Separating Declined/Cancelled"
      ],
      "metadata": {
        "id": "mj7s-SHWz9F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create parent category column\n",
        "def categorize_placement_status(status):\n",
        "    if status == 'Completed':\n",
        "        return 'Completed'\n",
        "    elif status in ['Cancelled-COVID', 'Cancelled-Weather', 'Cancelled-Illness']:\n",
        "        return 'Cancelled'\n",
        "    elif status in ['Initial Contact Made', 'Pending-Scheduling', 'Scheduled Interview',\n",
        "    'Internship In Process', 'Scheduled Event (Pending Completion)']:\n",
        "        return 'Pending'\n",
        "    else:\n",
        "        return 'Declined'\n",
        "\n",
        "dfmain['Placement Status Category'] = dfmain['Placement Status'].apply(categorize_placement_status)\n",
        "\n",
        "# Verify\n",
        "dfmain['Placement Status Category'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "v233isnh1PWy",
        "outputId": "0f7fa2f0-3ca5-4fcb-95aa-95652952cff5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Placement Status Category\n",
              "Completed    7549\n",
              "Declined     2161\n",
              "Pending       277\n",
              "Cancelled      81\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Placement Status Category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Completed</th>\n",
              "      <td>7549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Declined</th>\n",
              "      <td>2161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pending</th>\n",
              "      <td>277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cancelled</th>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Cancelled column and split Declined/Cancelled\n",
        "dfmain['Cancelled Student Interactions'] = 0\n",
        "dfmain = dfmain.rename(columns={\n",
        "    'Declined or Cancelled Student Interactions': 'Declined Student Interactions'\n",
        "})\n",
        "\n",
        "# Move Cancelled values to correct column based on Placement Status Category\n",
        "cancelled_mask = dfmain['Placement Status Category'] == 'Cancelled'\n",
        "dfmain.loc[cancelled_mask, 'Cancelled Student Interactions'] = dfmain.loc[cancelled_mask, 'Declined Student Interactions']\n",
        "dfmain.loc[cancelled_mask, 'Declined Student Interactions'] = 0\n",
        "\n",
        "# Convert 'Cancelled Student Interactions' to float datatype\n",
        "dfmain['Cancelled Student Interactions'] = dfmain['Cancelled Student Interactions'].astype(float)\n",
        "\n",
        "# Redefine numeric columns\n",
        "numeric_cols = [\n",
        "    'Complete Student Trainings',\n",
        "    'Complete Staff Trainings',\n",
        "    'Complete Student Interactions',\n",
        "    'Complete Student Internships',\n",
        "    'Internships in Progress',\n",
        "    'Pending Student Interactions',\n",
        "    'Declined Student Interactions',\n",
        "    'Cancelled Student Interactions',\n",
        "    'Pro101 Certificates Earned'\n",
        "]"
      ],
      "metadata": {
        "id": "Gu14y8tVPewQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pro101"
      ],
      "metadata": {
        "id": "W5PjFE61sB4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from rapidfuzz import fuzz\n",
        "\n",
        "# # Extract student name (before \"/\") from Student Sponsor Name\n",
        "# def extract_student_name(name):\n",
        "#     if pd.isna(name):\n",
        "#         return ''\n",
        "#     name_str = str(name)\n",
        "#     if '/' in name_str:\n",
        "#         return name_str.split('/')[0].strip()\n",
        "#     else:\n",
        "#         return name_str.strip()\n",
        "\n",
        "# # Add temporary column for student names\n",
        "# dfmain['Student Name'] = dfmain['Student Sponsor Name'].apply(extract_student_name)\n",
        "\n",
        "# # Find rows with Pro101 cert earned during other events\n",
        "# pro101_during_other_event = dfmain[\n",
        "#     (dfmain['Pro101 Certificates Earned'] > 0) &\n",
        "#     (dfmain['WBL Opportunity Type'] != 'Professionalism 101 Training')\n",
        "# ]\n",
        "\n",
        "# print(f\"Rows with Pro101 cert earned during OTHER events: {len(pro101_during_other_event)}\")\n",
        "\n",
        "# # Check if any already have matching standalone Pro101 rows\n",
        "# potential_duplicates = []\n",
        "\n",
        "# for idx, row in pro101_during_other_event.iterrows():\n",
        "#     # Get student name from this row\n",
        "#     student_name = row['Student Name']\n",
        "\n",
        "#     if not student_name:  # Skip if no student name\n",
        "#         continue\n",
        "\n",
        "#     # Look for Pro101 Training rows with fuzzy match on student name\n",
        "#     pro101_rows = dfmain[dfmain['WBL Opportunity Type'] == 'Professionalism 101 Training']\n",
        "\n",
        "#     for pro101_idx, pro101_row in pro101_rows.iterrows():\n",
        "#         pro101_student_name = pro101_row['Student Name']\n",
        "\n",
        "#         if not pro101_student_name:\n",
        "#             continue\n",
        "\n",
        "#         # Fuzzy match on student names\n",
        "#         similarity = fuzz.ratio(student_name.lower(), pro101_student_name.lower())\n",
        "\n",
        "#         if similarity >= 85:  # 85% threshold\n",
        "#             potential_duplicates.append((idx, pro101_idx, similarity))\n",
        "#             break  # Found a match, move to next row\n",
        "\n",
        "# print(f\"\\nRows that already have standalone Pro101 records: {len(potential_duplicates)}\")\n",
        "\n",
        "# if len(potential_duplicates) > 0:\n",
        "#     print(\"\\nSample matches (first 10):\")\n",
        "#     for orig_idx, pro101_idx, similarity in potential_duplicates[:10]:\n",
        "#         print(f\"\\nOriginal event row {orig_idx} matches Pro101 row {pro101_idx} (similarity: {similarity}%)\")\n",
        "#         print(f\"  Original: {dfmain.loc[orig_idx, 'Student Name']} - {dfmain.loc[orig_idx, 'WBL Opportunity Type']}\")\n",
        "#         print(f\"  Pro101:   {dfmain.loc[pro101_idx, 'Student Name']} - {dfmain.loc[pro101_idx, 'WBL Opportunity Type']}\")"
      ],
      "metadata": {
        "id": "Biud2DCf75_1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 2 existing Pro101 rows that we'll recreate from their matching events (found using fuzzy matching above)\n",
        "dfmain = dfmain.drop([1098, 540])\n",
        "\n",
        "# Find rows that need to be split (Pro101 earned during another event)\n",
        "rows_to_split = dfmain[\n",
        "    (dfmain['Pro101 Certificates Earned'] > 0) &\n",
        "    (dfmain['WBL Opportunity Type'] != 'Professionalism 101 Training')\n",
        "].copy()\n",
        "\n",
        "# Create new Pro101 rows\n",
        "new_pro101_rows = []\n",
        "\n",
        "for idx, row in rows_to_split.iterrows():\n",
        "    pro101_row = row.copy()\n",
        "    pro101_row['Placement Status'] = 'Completed'\n",
        "    pro101_row['Placement Status Category'] = 'Completed'\n",
        "    pro101_row['Business Champion Name'] = 'PPBEA'\n",
        "    pro101_row['Event Title'] = 'PPBEA Professionalism 101 Course'\n",
        "    pro101_row['WBL Opportunity Type'] = 'Professionalism 101 Training'\n",
        "    pro101_row['Complete Student Interactions'] = 1\n",
        "    pro101_row['Complete Student Trainings'] = 0\n",
        "    pro101_row['Complete Staff Trainings'] = 0\n",
        "    pro101_row['Complete Student Internships'] = 0\n",
        "    pro101_row['Internships in Progress'] = 0\n",
        "    pro101_row['Pending Student Interactions'] = 0\n",
        "    pro101_row['Declined Student Interactions'] = 0\n",
        "    pro101_row['Cancelled Student Interactions'] = 0\n",
        "    pro101_row['Pro101 Certificates Earned'] = 0\n",
        "    new_pro101_rows.append(pro101_row)\n",
        "\n",
        "# Add new Pro101 rows to dfmain\n",
        "dfmain = pd.concat([dfmain, pd.DataFrame(new_pro101_rows)], ignore_index=True)\n",
        "\n",
        "# Transfer 'Pro101 Certificated Earned' to 'Complete Student Interactions'\n",
        "pro101_completed_wrong = dfmain[\n",
        "    (dfmain['Placement Status'] == 'Completed') &\n",
        "    (dfmain['WBL Opportunity Type'] == 'Professionalism 101 Training') &\n",
        "    (dfmain['Complete Student Interactions'] == 0)\n",
        "]\n",
        "\n",
        "dfmain.loc[pro101_completed_wrong.index, 'Complete Student Interactions'] = 1\n",
        "\n",
        "# Drop Pro101 column (no longer needed)\n",
        "dfmain = dfmain.drop(columns=['Pro101 Certificates Earned'])\n",
        "\n",
        "print(f\"Created {len(new_pro101_rows)} new Pro101 rows\")\n",
        "dfmain.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbq3upO8KKFN",
        "outputId": "b952fc82-3991-47d3-df24-dc1ebdcebebd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 61 new Pro101 rows\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10127, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Placement Status & Numeric Columns Mismatch"
      ],
      "metadata": {
        "id": "Fc6r5G58MOf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "eBzt5Di-Xwag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefine numeric columns\n",
        "numeric_cols = [\n",
        "    'Complete Student Trainings',\n",
        "    'Complete Staff Trainings',\n",
        "    'Complete Student Interactions',\n",
        "    'Complete Student Internships',\n",
        "    'Internships in Progress',\n",
        "    'Pending Student Interactions',\n",
        "    'Declined Student Interactions',\n",
        "    'Cancelled Student Interactions',\n",
        "]\n",
        "\n",
        "# WBL to column mapping for Completed rows\n",
        "wbl_to_column_map_completed = {\n",
        "    'Staff Training': 'Complete Staff Trainings',\n",
        "    'Regional Advisory Meeting': 'Complete Staff Trainings',\n",
        "    'Site Visit - Staff': 'Complete Staff Trainings',\n",
        "    'Student Training': 'Complete Student Trainings',\n",
        "    'Professionalism 101 Training': 'Complete Student Interactions',\n",
        "    'Informational Interview Video': 'Complete Student Interactions',\n",
        "    'Career Story Video': 'Complete Student Interactions',\n",
        "    'e-WBL Informational Interview': 'Complete Student Interactions',\n",
        "    'e-WBL Class Presentation': 'Complete Student Interactions',\n",
        "    'Job Fair': 'Complete Student Interactions',\n",
        "    'Class/Group Mentorship': 'Complete Student Interactions',\n",
        "    'Industry Sponsored Project': 'Complete Student Interactions',\n",
        "    'Class Presentation': 'Complete Student Interactions',\n",
        "    'Job Shadow': 'Complete Student Interactions',\n",
        "    'Site Visit': 'Complete Student Interactions',\n",
        "    'Speakers Bureau': 'Complete Student Interactions',\n",
        "    'Event': 'Complete Student Interactions',\n",
        "    'Individual Mentorship': 'Complete Student Interactions',\n",
        "    'Paid Job': 'Complete Student Interactions',\n",
        "    'Internship 60': 'Complete Student Internships',\n",
        "    'Internship 120': 'Complete Student Internships',\n",
        "    'Internship 320': 'Complete Student Internships',\n",
        "    'Apprenticeship': 'Complete Student Interactions'\n",
        "}\n",
        "\n",
        "# WBL to column mapping for Pending rows\n",
        "wbl_to_column_map_pending = {\n",
        "    'Professionalism 101 Training': 'Pending Student Interactions',\n",
        "    'Career Story Video': 'Pending Student Interactions',\n",
        "    'e-WBL Informational Interview': 'Pending Student Interactions',\n",
        "    'Industry Sponsored Project': 'Pending Student Interactions',\n",
        "    'Class Presentation': 'Pending Student Interactions',\n",
        "    'Job Shadow': 'Pending Student Interactions',\n",
        "    'Site Visit': 'Pending Student Interactions',\n",
        "    'Speakers Bureau': 'Pending Student Interactions',\n",
        "    'Event': 'Pending Student Interactions',\n",
        "    'Internship 60': 'Internships in Progress',\n",
        "    'Internship 120': 'Internships in Progress',\n",
        "    'Internship 320': 'Internships in Progress',\n",
        "    'Apprenticeship': 'Pending Student Interactions'\n",
        "}"
      ],
      "metadata": {
        "id": "530B_6RFXxEr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fixing Double-Counting"
      ],
      "metadata": {
        "id": "hxYS8JQQVQyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix double-counting: rows with values in multiple numeric columns\n",
        "dfmain['num_cols_with_values'] = (dfmain[numeric_cols] > 0).sum(axis=1)\n",
        "rows_with_multiple = dfmain['num_cols_with_values'] > 1\n",
        "\n",
        "# Zero out Complete Student Interactions for rows with Trainings + Interactions\n",
        "dfmain.loc[rows_with_multiple, 'Complete Student Interactions'] = 0\n",
        "\n",
        "# Drop helper column\n",
        "dfmain = dfmain.drop(columns=['num_cols_with_values'])\n",
        "\n",
        "# Create a column for numeric sum\n",
        "dfmain['_numeric_sum'] = dfmain[numeric_cols].sum(axis=1)"
      ],
      "metadata": {
        "id": "Z8vpk5my3EJc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify 'Placement Status' Matches Numeric Column Values"
      ],
      "metadata": {
        "id": "6TceInt1VXko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate Placement Status & Numeric Column Mismatches\n",
        "\n",
        "# Define column groups by placement status category\n",
        "complete_cols = ['Complete Student Trainings', 'Complete Staff Trainings',\n",
        "                 'Complete Student Interactions', 'Complete Student Internships']\n",
        "declined_cols = ['Declined Student Interactions']\n",
        "cancelled_cols = ['Cancelled Student Interactions']\n",
        "pending_cols = ['Pending Student Interactions', 'Internships in Progress']\n",
        "\n",
        "# All numeric columns\n",
        "all_numeric_cols = complete_cols + declined_cols + cancelled_cols + pending_cols\n",
        "\n",
        "# Find mismatches\n",
        "mismatches = []\n",
        "\n",
        "for idx in dfmain.index:\n",
        "    row = dfmain.loc[idx]\n",
        "\n",
        "    # Skip all-zero rows (handled separately in fill-all-zeros section)\n",
        "    if (row[all_numeric_cols] == 0).all():\n",
        "        continue\n",
        "\n",
        "    category = row['Placement Status Category']\n",
        "\n",
        "    # Check what columns have values\n",
        "    has_complete = (row[complete_cols] > 0).any()\n",
        "    has_declined = row['Declined Student Interactions'] > 0\n",
        "    has_cancelled = row['Cancelled Student Interactions'] > 0\n",
        "    has_pending = (row[pending_cols] > 0).any()\n",
        "\n",
        "    # Determine if there's a mismatch\n",
        "    is_mismatch = False\n",
        "    mismatch_type = None\n",
        "\n",
        "    if category == 'Completed':\n",
        "        if has_declined or has_cancelled or has_pending:\n",
        "            is_mismatch = True\n",
        "            mismatch_type = 'Completed status but has Declined/Cancelled/Pending values'\n",
        "        elif not has_complete:\n",
        "            is_mismatch = True\n",
        "            mismatch_type = 'Completed status but no Complete values'\n",
        "\n",
        "    elif category == 'Declined':\n",
        "        if has_complete or has_cancelled or has_pending:\n",
        "            is_mismatch = True\n",
        "            mismatch_type = 'Declined status but has Complete/Cancelled/Pending values'\n",
        "        elif not has_declined:\n",
        "            is_mismatch = True\n",
        "            mismatch_type = 'Declined status but no Declined values'\n",
        "\n",
        "    elif category == 'Cancelled':\n",
        "        if has_complete or has_declined or has_pending:\n",
        "            is_mismatch = True\n",
        "            mismatch_type = 'Cancelled status but has Complete/Declined/Pending values'\n",
        "        elif not has_cancelled:\n",
        "            is_mismatch = True\n",
        "            mismatch_type = 'Cancelled status but no Cancelled values'\n",
        "\n",
        "    elif category == 'Pending':\n",
        "        if has_complete or has_declined or has_cancelled:\n",
        "            is_mismatch = True\n",
        "            mismatch_type = 'Pending status but has Complete/Declined/Cancelled values'\n",
        "        elif not has_pending:\n",
        "            is_mismatch = True\n",
        "            mismatch_type = 'Pending status but no Pending values'\n",
        "\n",
        "    if is_mismatch:\n",
        "        mismatches.append({\n",
        "            'index': idx,\n",
        "            'Placement Status': row['Placement Status'],\n",
        "            'Placement Status Category': category,\n",
        "            'Mismatch Type': mismatch_type,\n",
        "            'Complete Student Trainings': row['Complete Student Trainings'],\n",
        "            'Complete Staff Trainings': row['Complete Staff Trainings'],\n",
        "            'Complete Student Interactions': row['Complete Student Interactions'],\n",
        "            'Complete Student Internships': row['Complete Student Internships'],\n",
        "            'Declined Student Interactions': row['Declined Student Interactions'],\n",
        "            'Cancelled Student Interactions': row['Cancelled Student Interactions'],\n",
        "            'Pending Student Interactions': row['Pending Student Interactions'],\n",
        "            'Internships in Progress': row['Internships in Progress'],\n",
        "            'WBL Opportunity Type': row['WBL Opportunity Type'],\n",
        "            'School Year': row['School Year']\n",
        "        })\n",
        "\n",
        "# Create dataframe\n",
        "if len(mismatches) > 0:\n",
        "    mismatches_df = pd.DataFrame(mismatches)\n",
        "\n",
        "    print(f\"Total mismatches found (excluding all-zero rows): {len(mismatches_df)}\")\n",
        "    print(\"\\nBreakdown by Mismatch Type:\")\n",
        "    print(mismatches_df['Mismatch Type'].value_counts())\n",
        "\n",
        "    print(\"\\nBreakdown by Placement Status Category:\")\n",
        "    print(mismatches_df['Placement Status Category'].value_counts())\n",
        "\n",
        "    print(\"\\nBreakdown by School Year:\")\n",
        "    print(mismatches_df['School Year'].value_counts())\n",
        "\n",
        "    # print(\"\\nFirst 20 mismatches:\")\n",
        "    # display(mismatches_df.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ddLrnMhVQLE",
        "outputId": "1c2aa1c6-36c9-4ca3-c511-666c56f897b1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total mismatches found (excluding all-zero rows): 30\n",
            "\n",
            "Breakdown by Mismatch Type:\n",
            "Mismatch Type\n",
            "Pending status but has Complete/Declined/Cancelled values     28\n",
            "Completed status but has Declined/Cancelled/Pending values     2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Breakdown by Placement Status Category:\n",
            "Placement Status Category\n",
            "Pending      28\n",
            "Completed     2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Breakdown by School Year:\n",
            "School Year\n",
            "2019-2020    28\n",
            "2021-2022     1\n",
            "2023-2024     1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix Placement Status & Numeric Column Mismatches\n",
        "\n",
        "# Track fixes\n",
        "fixed_rows = []\n",
        "\n",
        "# Fix the mismatches\n",
        "for idx in mismatches_df['index']:\n",
        "    row = dfmain.loc[idx]\n",
        "    category = row['Placement Status Category']\n",
        "    wbl_type = row['WBL Opportunity Type']\n",
        "\n",
        "    # Check what columns have values\n",
        "    has_complete = (row[complete_cols] > 0).any()\n",
        "    has_declined = row['Declined Student Interactions'] > 0\n",
        "    has_cancelled = row['Cancelled Student Interactions'] > 0\n",
        "    has_pending = (row[pending_cols] > 0).any()\n",
        "\n",
        "    # Case 1: Pending status but has Complete/Declined/Cancelled values\n",
        "    if category == 'Pending' and (has_complete or has_declined or has_cancelled):\n",
        "        # Change to Completed status\n",
        "        dfmain.loc[idx, 'Placement Status'] = 'Completed'\n",
        "        dfmain.loc[idx, 'Placement Status Category'] = 'Completed'\n",
        "\n",
        "        fixed_rows.append({\n",
        "            'index': idx,\n",
        "            'Fix Type': 'Changed Pending → Completed',\n",
        "            'WBL Opportunity Type': wbl_type,\n",
        "            'Old Status': 'Pending',\n",
        "            'New Status': 'Completed'\n",
        "        })\n",
        "\n",
        "    # Case 2: Completed status but has Declined/Cancelled values\n",
        "    elif category == 'Completed' and (has_declined or has_cancelled):\n",
        "        # Move values to correct Complete column based on WBL mapping\n",
        "        if wbl_type in wbl_to_column_map_completed:\n",
        "            correct_column = wbl_to_column_map_completed[wbl_type]\n",
        "\n",
        "            # Move Declined value if present\n",
        "            if has_declined:\n",
        "                value = row['Declined Student Interactions']\n",
        "                dfmain.loc[idx, correct_column] = dfmain.loc[idx, correct_column] + value\n",
        "                dfmain.loc[idx, 'Declined Student Interactions'] = 0\n",
        "\n",
        "                fixed_rows.append({\n",
        "                    'index': idx,\n",
        "                    'Fix Type': 'Moved Declined → Complete',\n",
        "                    'WBL Opportunity Type': wbl_type,\n",
        "                    'Value Moved': value,\n",
        "                    'To Column': correct_column\n",
        "                })\n",
        "\n",
        "            # Move Cancelled value if present\n",
        "            if has_cancelled:\n",
        "                value = row['Cancelled Student Interactions']\n",
        "                dfmain.loc[idx, correct_column] = dfmain.loc[idx, correct_column] + value\n",
        "                dfmain.loc[idx, 'Cancelled Student Interactions'] = 0\n",
        "\n",
        "                fixed_rows.append({\n",
        "                    'index': idx,\n",
        "                    'Fix Type': 'Moved Cancelled → Complete',\n",
        "                    'WBL Opportunity Type': wbl_type,\n",
        "                    'Value Moved': value,\n",
        "                    'To Column': correct_column\n",
        "                })\n",
        "\n",
        "# # Create summary dataframe\n",
        "# if len(fixed_rows) > 0:\n",
        "#     fixed_df = pd.DataFrame(fixed_rows)\n",
        "#     print(f\"Fixed {len(fixed_rows)} placement status mismatches\")\n",
        "#     print(\"\\nBreakdown by Fix Type:\")\n",
        "#     print(fixed_df['Fix Type'].value_counts())\n",
        "#     print(\"\\nFixed rows:\")\n",
        "#     display(fixed_df)\n",
        "# else:\n",
        "#     print(\"No fixes needed\")"
      ],
      "metadata": {
        "id": "8uN4s2gNY3YE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verifying Correct Mapping from WBL Type to Respective Numeric Column"
      ],
      "metadata": {
        "id": "lAtMOdjklrPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the complete columns\n",
        "# complete_cols = ['Complete Student Trainings', 'Complete Student Interactions',\n",
        "#                  'Complete Student Internships']\n",
        "\n",
        "# # Analyze Completed rows\n",
        "# completed_rows = dfmain[dfmain['Placement Status Category'] == 'Completed'].copy()\n",
        "\n",
        "# print(\"=\"*100)\n",
        "# print(\"COMPLETED ROWS - WBL Type to Numeric Column Distribution\")\n",
        "# print(\"=\"*100)\n",
        "\n",
        "# for wbl_type in sorted(completed_rows['WBL Opportunity Type'].dropna().unique()):\n",
        "#     wbl_subset = completed_rows[completed_rows['WBL Opportunity Type'] == wbl_type]\n",
        "\n",
        "#     # Only show if there are non-zero values\n",
        "#     has_values = (wbl_subset[complete_cols] > 0).any().any()\n",
        "\n",
        "#     if has_values:\n",
        "#         print(f\"\\n{wbl_type} ({len(wbl_subset)} rows):\")\n",
        "\n",
        "#         for col in complete_cols:\n",
        "#             count = (wbl_subset[col] > 0).sum()\n",
        "#             if count > 0:\n",
        "#                 percentage = (count / len(wbl_subset)) * 100\n",
        "#                 print(f\"  {col}: {count} rows ({percentage:.1f}%)\")\n",
        "\n",
        "# # Analyze Pending rows if they exist\n",
        "# pending_cols = ['Pending Student Interactions', 'Internships in Progress']\n",
        "# pending_rows = dfmain[dfmain['Placement Status Category'] == 'Pending']\n",
        "\n",
        "# if len(pending_rows) > 0:\n",
        "#     print(\"\\n\" + \"=\"*100)\n",
        "#     print(\"PENDING ROWS - WBL Type to Numeric Column Distribution\")\n",
        "#     print(\"=\"*100)\n",
        "\n",
        "#     for wbl_type in sorted(pending_rows['WBL Opportunity Type'].dropna().unique()):\n",
        "#         wbl_subset = pending_rows[pending_rows['WBL Opportunity Type'] == wbl_type]\n",
        "\n",
        "#         # Only show if there are non-zero values\n",
        "#         has_values = (wbl_subset[pending_cols] > 0).any().any()\n",
        "\n",
        "#         if has_values:\n",
        "#             print(f\"\\n{wbl_type} ({len(wbl_subset)} rows):\")\n",
        "\n",
        "#             for col in pending_cols:\n",
        "#                 count = (wbl_subset[col] > 0).sum()\n",
        "#                 if count > 0:\n",
        "#                     percentage = (count / len(wbl_subset)) * 100\n",
        "#                     print(f\"  {col}: {count} rows ({percentage:.1f}%)\")"
      ],
      "metadata": {
        "id": "VyD_KRv-sMy7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the complete columns we're checking\n",
        "complete_cols = ['Complete Student Trainings', 'Complete Student Interactions',\n",
        "                 'Complete Student Internships']\n",
        "\n",
        "# Track remapped rows\n",
        "remapped_rows = []\n",
        "\n",
        "# Process Completed rows\n",
        "completed_rows = dfmain['Placement Status Category'] == 'Completed'\n",
        "\n",
        "for idx in dfmain[completed_rows].index:\n",
        "    row = dfmain.loc[idx].copy()\n",
        "    wbl_type = row['WBL Opportunity Type']\n",
        "\n",
        "    if wbl_type not in wbl_to_column_map_completed:\n",
        "        continue\n",
        "\n",
        "    correct_column = wbl_to_column_map_completed[wbl_type]\n",
        "\n",
        "    # Check if any wrong columns have values\n",
        "    for col in complete_cols:\n",
        "        if col != correct_column and row[col] > 0:\n",
        "            # EXCEPTION: Allow both Complete Staff Trainings and Complete Student Trainings\n",
        "            if {col, correct_column} == {'Complete Staff Trainings', 'Complete Student Trainings'}:\n",
        "                continue  # Skip this - it's a valid combination\n",
        "\n",
        "            # Record the change\n",
        "            remapped_rows.append({\n",
        "                'index': idx,\n",
        "                'WBL Opportunity Type': wbl_type,\n",
        "                'Placement Status Category': 'Completed',\n",
        "                'Wrong Column': col,\n",
        "                'Correct Column': correct_column,\n",
        "                'Value Moved': row[col],\n",
        "                'Before': {c: row[c] for c in complete_cols},\n",
        "            })\n",
        "\n",
        "            # Move value from wrong column to correct column\n",
        "            value = row[col]\n",
        "            dfmain.loc[idx, correct_column] = dfmain.loc[idx, correct_column] + value\n",
        "            dfmain.loc[idx, col] = 0\n",
        "\n",
        "# Process Pending rows\n",
        "pending_cols = ['Pending Student Interactions', 'Internships in Progress']\n",
        "pending_rows = dfmain['Placement Status Category'] == 'Pending'\n",
        "\n",
        "for idx in dfmain[pending_rows].index:\n",
        "    row = dfmain.loc[idx].copy()\n",
        "    wbl_type = row['WBL Opportunity Type']\n",
        "\n",
        "    if wbl_type not in wbl_to_column_map_pending:\n",
        "        continue\n",
        "\n",
        "    correct_column = wbl_to_column_map_pending[wbl_type]\n",
        "\n",
        "    # Check if any wrong columns have values\n",
        "    for col in pending_cols:\n",
        "        if col != correct_column and row[col] > 0:\n",
        "            # Record the change\n",
        "            remapped_rows.append({\n",
        "                'index': idx,\n",
        "                'WBL Opportunity Type': wbl_type,\n",
        "                'Placement Status Category': 'Pending',\n",
        "                'Wrong Column': col,\n",
        "                'Correct Column': correct_column,\n",
        "                'Value Moved': row[col],\n",
        "                'Before': {c: row[c] for c in pending_cols},\n",
        "            })\n",
        "\n",
        "            # Move value from wrong column to correct column\n",
        "            value = row[col]\n",
        "            dfmain.loc[idx, correct_column] = dfmain.loc[idx, correct_column] + value\n",
        "            dfmain.loc[idx, col] = 0\n",
        "\n",
        "# # Create dataframe of remapped rows\n",
        "# if len(remapped_rows) > 0:\n",
        "#     remapped_df = pd.DataFrame(remapped_rows)\n",
        "#     print(f\"Remapped {len(remapped_rows)} rows to correct numeric columns\")\n",
        "#     print(\"\\nRemapped rows:\")\n",
        "#     display(remapped_df)\n",
        "# else:\n",
        "#     print(\"No rows needed remapping - all values already in correct columns!\")"
      ],
      "metadata": {
        "id": "zFgvgLVFqEbL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fill All-Zero Rows with Respective Values"
      ],
      "metadata": {
        "id": "TkzfvSGz4dGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main"
      ],
      "metadata": {
        "id": "DGMnlXtXvXc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all-zero rows\n",
        "all_zeros = (dfmain[numeric_cols] == 0).all(axis=1)\n",
        "\n",
        "# Get WBL types that have all-zero rows\n",
        "wbl_types_with_zeros = dfmain[all_zeros]['WBL Opportunity Type'].unique()\n",
        "\n",
        "# Show value counts of WBL Opportunity Type for all-zero rows\n",
        "print(\"\\nWBL Opportunity Type breakdown for all-zero rows:\")\n",
        "print(dfmain[all_zeros]['WBL Opportunity Type'].value_counts())\n",
        "print(f\"\\nTotal all-zero rows: {all_zeros.sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX3hbpXk4SHL",
        "outputId": "e55a18d1-6d8a-41cb-a8eb-4bf9699c3d96"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WBL Opportunity Type breakdown for all-zero rows:\n",
            "WBL Opportunity Type\n",
            "Speakers Bureau                  489\n",
            "Professionalism 101 Training     141\n",
            "Site Visit                       134\n",
            "Class Presentation                98\n",
            "e-WBL Class Presentation          91\n",
            "Internship 60                     62\n",
            "Regional Advisory Meeting         42\n",
            "Student Training                   9\n",
            "Job Shadow                         8\n",
            "Event                              5\n",
            "e-WBL Informational Interview      5\n",
            "Site Visit - Staff                 4\n",
            "Industry Sponsored Project         1\n",
            "Career Story Video                 1\n",
            "Apprenticeship                     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Total all-zero rows: 1091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All 1's"
      ],
      "metadata": {
        "id": "ZnEEK6Ply9Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy to test this method\n",
        "dfmain_test = dfmain.copy()\n",
        "\n",
        "# Fill all-zero rows with 1's\n",
        "for idx in dfmain_test[all_zeros].index:\n",
        "    row = dfmain_test.loc[idx]\n",
        "    wbl_type = row['WBL Opportunity Type']\n",
        "    category = row['Placement Status Category']\n",
        "\n",
        "    if category == 'Completed':\n",
        "        if wbl_type in wbl_to_column_map_completed:\n",
        "            dfmain_test.loc[idx, wbl_to_column_map_completed[wbl_type]] = 1\n",
        "    elif category == 'Pending':\n",
        "        if wbl_type in wbl_to_column_map_pending:\n",
        "            dfmain_test.loc[idx, wbl_to_column_map_pending[wbl_type]] = 1\n",
        "    elif category == 'Declined':\n",
        "        dfmain_test.loc[idx, 'Declined Student Interactions'] = 1\n",
        "    elif category == 'Cancelled':\n",
        "        dfmain_test.loc[idx, 'Cancelled Student Interactions'] = 1\n",
        "\n",
        "# Calculate metrics\n",
        "total_completed = dfmain_test['Complete Student Interactions'].sum() + dfmain_test['Complete Student Trainings'].sum() + dfmain_test['Complete Student Internships'].sum()\n",
        "total_declined = dfmain_test['Declined Student Interactions'].sum()\n",
        "total_cancelled = dfmain_test['Cancelled Student Interactions'].sum()\n",
        "grand_total = total_completed + total_declined + total_cancelled\n",
        "\n",
        "# Store results\n",
        "method_1_results = {\n",
        "    'Method': 'All 1s',\n",
        "    'Total Completed': total_completed,\n",
        "    'Total Declined': total_declined,\n",
        "    'Total Cancelled': total_cancelled,\n",
        "    'Grand Total': grand_total\n",
        "}"
      ],
      "metadata": {
        "id": "flqAdN37y--E"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Min"
      ],
      "metadata": {
        "id": "pYxMqb3gvhzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy to test this method\n",
        "dfmain_test = dfmain.copy()\n",
        "\n",
        "# Calculate minimum numeric sum by WBL Opportunity Type (excluding zeros)\n",
        "wbl_minimums = dfmain_test[dfmain_test['_numeric_sum'] > 0].groupby('WBL Opportunity Type')['_numeric_sum'].min()\n",
        "\n",
        "print(\"Minimum values by WBL Opportunity Type (for types with all-zero rows):\")\n",
        "for wbl_type in sorted(wbl_types_with_zeros):\n",
        "    if pd.notna(wbl_type):\n",
        "        min_val = wbl_minimums.get(wbl_type, 1)\n",
        "        print(f\"  {wbl_type}: {min_val}\")\n",
        "\n",
        "# Fill all-zero rows with minimum values\n",
        "for idx in dfmain_test[all_zeros].index:\n",
        "    row = dfmain_test.loc[idx]\n",
        "    wbl_type = row['WBL Opportunity Type']\n",
        "    category = row['Placement Status Category']\n",
        "\n",
        "    min_value = wbl_minimums.get(wbl_type, 1)\n",
        "\n",
        "    if category == 'Completed':\n",
        "        if wbl_type in wbl_to_column_map_completed:\n",
        "            dfmain_test.loc[idx, wbl_to_column_map_completed[wbl_type]] = min_value\n",
        "    elif category == 'Pending':\n",
        "        if wbl_type in wbl_to_column_map_pending:\n",
        "            dfmain_test.loc[idx, wbl_to_column_map_pending[wbl_type]] = min_value\n",
        "    elif category == 'Declined':\n",
        "        dfmain_test.loc[idx, 'Declined Student Interactions'] = min_value\n",
        "    elif category == 'Cancelled':\n",
        "        dfmain_test.loc[idx, 'Cancelled Student Interactions'] = min_value\n",
        "\n",
        "# Calculate metrics\n",
        "total_completed = dfmain_test['Complete Student Interactions'].sum() + dfmain_test['Complete Student Trainings'].sum() + dfmain_test['Complete Student Internships'].sum()\n",
        "total_declined = dfmain_test['Declined Student Interactions'].sum()\n",
        "total_cancelled = dfmain_test['Cancelled Student Interactions'].sum()\n",
        "grand_total = total_completed + total_declined + total_cancelled\n",
        "\n",
        "# Store results\n",
        "method_2_results = {\n",
        "    'Method': 'Minimum',\n",
        "    'Total Completed': total_completed,\n",
        "    'Total Declined': total_declined,\n",
        "    'Total Cancelled': total_cancelled,\n",
        "    'Grand Total': grand_total\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRfqAWPBzdwY",
        "outputId": "eaca2d0b-d100-4dc9-e64f-3041fae0e9fc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum values by WBL Opportunity Type (for types with all-zero rows):\n",
            "  Apprenticeship: 1.0\n",
            "  Career Story Video: 1.0\n",
            "  Class Presentation: 1.0\n",
            "  Event: 1.0\n",
            "  Industry Sponsored Project: 1.0\n",
            "  Internship 60: 1.0\n",
            "  Job Shadow: 1.0\n",
            "  Professionalism 101 Training: 1.0\n",
            "  Regional Advisory Meeting: 1\n",
            "  Site Visit: 1.0\n",
            "  Site Visit - Staff: 1\n",
            "  Speakers Bureau: 10.0\n",
            "  Student Training: 1.0\n",
            "  e-WBL Class Presentation: 1.0\n",
            "  e-WBL Informational Interview: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 25th Quartile"
      ],
      "metadata": {
        "id": "xxbQBcXZvjdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy to test this method\n",
        "dfmain_test = dfmain.copy()\n",
        "\n",
        "# Calculate 25th percentile numeric sum by WBL Opportunity Type (excluding zeros)\n",
        "wbl_25th_percentile = dfmain_test[dfmain_test['_numeric_sum'] > 0].groupby('WBL Opportunity Type')['_numeric_sum'].quantile(0.25)\n",
        "\n",
        "print(\"25th Percentile values by WBL Opportunity Type (for types with all-zero rows):\")\n",
        "for wbl_type in sorted(wbl_types_with_zeros):\n",
        "    if pd.notna(wbl_type):\n",
        "        percentile_val = wbl_25th_percentile.get(wbl_type, 1)\n",
        "        print(f\"  {wbl_type}: {percentile_val}\")\n",
        "\n",
        "# Fill all-zero rows with 25th percentile values\n",
        "for idx in dfmain_test[all_zeros].index:\n",
        "    row = dfmain_test.loc[idx]\n",
        "    wbl_type = row['WBL Opportunity Type']\n",
        "    category = row['Placement Status Category']\n",
        "\n",
        "    percentile_value = wbl_25th_percentile.get(wbl_type, 1)\n",
        "\n",
        "    if category == 'Completed':\n",
        "        if wbl_type in wbl_to_column_map_completed:\n",
        "            dfmain_test.loc[idx, wbl_to_column_map_completed[wbl_type]] = percentile_value\n",
        "    elif category == 'Pending':\n",
        "        if wbl_type in wbl_to_column_map_pending:\n",
        "            dfmain_test.loc[idx, wbl_to_column_map_pending[wbl_type]] = percentile_value\n",
        "    elif category == 'Declined':\n",
        "        dfmain_test.loc[idx, 'Declined Student Interactions'] = percentile_value\n",
        "    elif category == 'Cancelled':\n",
        "        dfmain_test.loc[idx, 'Cancelled Student Interactions'] = percentile_value\n",
        "\n",
        "# Calculate metrics\n",
        "total_completed = dfmain_test['Complete Student Interactions'].sum() + dfmain_test['Complete Student Trainings'].sum() + dfmain_test['Complete Student Internships'].sum()\n",
        "total_declined = dfmain_test['Declined Student Interactions'].sum()\n",
        "total_cancelled = dfmain_test['Cancelled Student Interactions'].sum()\n",
        "grand_total = total_completed + total_declined + total_cancelled\n",
        "\n",
        "# Store results\n",
        "method_3_results = {\n",
        "    'Method': '25th Percentile',\n",
        "    'Total Completed': total_completed,\n",
        "    'Total Declined': total_declined,\n",
        "    'Total Cancelled': total_cancelled,\n",
        "    'Grand Total': grand_total\n",
        "}"
      ],
      "metadata": {
        "id": "agtyiY2xvpOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b49cb15-922a-4a60-abe7-42705feb08d0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25th Percentile values by WBL Opportunity Type (for types with all-zero rows):\n",
            "  Apprenticeship: 1.0\n",
            "  Career Story Video: 1.0\n",
            "  Class Presentation: 15.0\n",
            "  Event: 6.0\n",
            "  Industry Sponsored Project: 1.0\n",
            "  Internship 60: 1.0\n",
            "  Job Shadow: 1.0\n",
            "  Professionalism 101 Training: 1.0\n",
            "  Regional Advisory Meeting: 1\n",
            "  Site Visit: 10.0\n",
            "  Site Visit - Staff: 1\n",
            "  Speakers Bureau: 35.0\n",
            "  Student Training: 3.0\n",
            "  e-WBL Class Presentation: 4.0\n",
            "  e-WBL Informational Interview: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Median"
      ],
      "metadata": {
        "id": "bxuRiqyovmfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy to test this method\n",
        "dfmain_test = dfmain.copy()\n",
        "\n",
        "# Calculate median numeric sum by WBL Opportunity Type (excluding zeros)\n",
        "wbl_medians = dfmain_test[dfmain_test['_numeric_sum'] > 0].groupby('WBL Opportunity Type')['_numeric_sum'].median()\n",
        "\n",
        "print(\"Median values by WBL Opportunity Type (for types with all-zero rows):\")\n",
        "for wbl_type in sorted(wbl_types_with_zeros):\n",
        "    if pd.notna(wbl_type):\n",
        "        median_val = wbl_medians.get(wbl_type, 1)\n",
        "        print(f\"  {wbl_type}: {median_val}\")\n",
        "\n",
        "# Fill all-zero rows with median values\n",
        "for idx in dfmain_test[all_zeros].index:\n",
        "    row = dfmain_test.loc[idx]\n",
        "    wbl_type = row['WBL Opportunity Type']\n",
        "    category = row['Placement Status Category']\n",
        "\n",
        "    median_value = wbl_medians.get(wbl_type, 1)\n",
        "\n",
        "    if category == 'Completed':\n",
        "        if wbl_type in wbl_to_column_map_completed:\n",
        "            dfmain_test.loc[idx, wbl_to_column_map_completed[wbl_type]] = median_value\n",
        "    elif category == 'Pending':\n",
        "        if wbl_type in wbl_to_column_map_pending:\n",
        "            dfmain_test.loc[idx, wbl_to_column_map_pending[wbl_type]] = median_value\n",
        "    elif category == 'Declined':\n",
        "        dfmain_test.loc[idx, 'Declined Student Interactions'] = median_value\n",
        "    elif category == 'Cancelled':\n",
        "        dfmain_test.loc[idx, 'Cancelled Student Interactions'] = median_value\n",
        "\n",
        "# Calculate metrics\n",
        "total_completed = dfmain_test['Complete Student Interactions'].sum() + dfmain_test['Complete Student Trainings'].sum() + dfmain_test['Complete Student Internships'].sum()\n",
        "total_declined = dfmain_test['Declined Student Interactions'].sum()\n",
        "total_cancelled = dfmain_test['Cancelled Student Interactions'].sum()\n",
        "grand_total = total_completed + total_declined + total_cancelled\n",
        "\n",
        "# Store results\n",
        "method_4_results = {\n",
        "    'Method': 'Median',\n",
        "    'Total Completed': total_completed,\n",
        "    'Total Declined': total_declined,\n",
        "    'Total Cancelled': total_cancelled,\n",
        "    'Grand Total': grand_total\n",
        "}"
      ],
      "metadata": {
        "id": "tu0JOno0vrXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7a7a10-341d-40e1-98b3-d2b3a1a2e932"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Median values by WBL Opportunity Type (for types with all-zero rows):\n",
            "  Apprenticeship: 1.0\n",
            "  Career Story Video: 1.0\n",
            "  Class Presentation: 25.0\n",
            "  Event: 15.0\n",
            "  Industry Sponsored Project: 5.5\n",
            "  Internship 60: 1.0\n",
            "  Job Shadow: 1.0\n",
            "  Professionalism 101 Training: 1.0\n",
            "  Regional Advisory Meeting: 1\n",
            "  Site Visit: 15.0\n",
            "  Site Visit - Staff: 1\n",
            "  Speakers Bureau: 51.0\n",
            "  Student Training: 20.0\n",
            "  e-WBL Class Presentation: 10.0\n",
            "  e-WBL Informational Interview: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare all methods side-by-side"
      ],
      "metadata": {
        "id": "xgbnc-ziwWD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all results into a comparison dataframe\n",
        "comparison_df = pd.DataFrame([\n",
        "    method_1_results,\n",
        "    method_2_results,\n",
        "    method_3_results,\n",
        "    method_4_results\n",
        "])\n",
        "\n",
        "# Set Method as index for cleaner display\n",
        "comparison_df = comparison_df.set_index('Method')\n",
        "\n",
        "print(\"=\"*100)\n",
        "print(\"FILLING METHOD COMPARISON - STUDENT ENGAGEMENT METRICS\")\n",
        "print(\"=\"*100)\n",
        "print(\"\\n\")\n",
        "print(comparison_df.to_string())\n",
        "\n",
        "# Show differences from minimum method (baseline)\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"DIFFERENCES FROM 'ALL 1s' METHOD (Baseline)\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "baseline = comparison_df.loc['All 1s']\n",
        "differences = comparison_df - baseline\n",
        "\n",
        "print(\"\\n\")\n",
        "print(differences.to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrzEYJat2YaT",
        "outputId": "51c5282c-89b5-453d-8299-98fb04e7150f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "FILLING METHOD COMPARISON - STUDENT ENGAGEMENT METRICS\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "                 Total Completed  Total Declined  Total Cancelled  Grand Total\n",
            "Method                                                                        \n",
            "All 1s                   50836.0          4456.0           1737.0      57029.0\n",
            "Minimum                  50935.0          8668.0           1764.0      61367.0\n",
            "25th Percentile          51596.0         22782.0           1855.0      76233.0\n",
            "Median                   52427.0         31932.0           1930.0      86289.0\n",
            "\n",
            "====================================================================================================\n",
            "DIFFERENCES FROM 'ALL 1s' METHOD (Baseline)\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "                 Total Completed  Total Declined  Total Cancelled  Grand Total\n",
            "Method                                                                        \n",
            "All 1s                       0.0             0.0              0.0          0.0\n",
            "Minimum                     99.0          4212.0             27.0       4338.0\n",
            "25th Percentile            760.0         18326.0            118.0      19204.0\n",
            "Median                    1591.0         27476.0            193.0      29260.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Current Method: Minimum + Median\n",
        "\n",
        "# Calculate median numeric sum by WBL Opportunity Type (excluding zeros)\n",
        "wbl_medians = dfmain[dfmain['_numeric_sum'] > 0].groupby('WBL Opportunity Type')['_numeric_sum'].median()\n",
        "\n",
        "# Calculate minimum numeric sum by WBL Opportunity Type (excluding zeros)\n",
        "wbl_minimums = dfmain[dfmain['_numeric_sum'] > 0].groupby('WBL Opportunity Type')['_numeric_sum'].min()\n",
        "\n",
        "# Fill all-zero rows with median values\n",
        "for idx in dfmain[all_zeros].index:\n",
        "    row = dfmain.loc[idx]\n",
        "    wbl_type = row['WBL Opportunity Type']\n",
        "    category = row['Placement Status Category']\n",
        "\n",
        "    median_value = wbl_medians.get(wbl_type, 1)\n",
        "\n",
        "    min_value = wbl_minimums.get(wbl_type, 1)\n",
        "\n",
        "    if category == 'Completed':\n",
        "        if wbl_type in wbl_to_column_map_completed:\n",
        "            dfmain.loc[idx, wbl_to_column_map_completed[wbl_type]] = median_value\n",
        "    elif category == 'Pending':\n",
        "        if wbl_type in wbl_to_column_map_pending:\n",
        "            dfmain.loc[idx, wbl_to_column_map_pending[wbl_type]] = min_value\n",
        "    elif category == 'Declined':\n",
        "        dfmain.loc[idx, 'Declined Student Interactions'] = min_value\n",
        "    elif category == 'Cancelled':\n",
        "        dfmain.loc[idx, 'Cancelled Student Interactions'] = min_value"
      ],
      "metadata": {
        "id": "7oS1aYfNMi1Z"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop temporary numeric sum column\n",
        "dfmain = dfmain.drop(columns=['_numeric_sum'])"
      ],
      "metadata": {
        "id": "jl5b1thlwcJ3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify\n",
        "all_zeros_after = (dfmain[numeric_cols] == 0).all(axis=1)\n",
        "print(f\"\\nAll-zero rows before filling: {all_zeros.sum()}\")\n",
        "print(f\"All-zero rows after filling: {all_zeros_after.sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZYw1nWpwaYh",
        "outputId": "ba092117-dd1d-41ba-8809-81c0880022b8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All-zero rows before filling: 1091\n",
            "All-zero rows after filling: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Duplicate Handling & Pending Events"
      ],
      "metadata": {
        "id": "ZPZYqUnN3Nga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base matching fields for duplicate detection\n",
        "base_match_fields = [\n",
        "    'Business Champion Name',\n",
        "    'Student Sponsor Name',\n",
        "    'WBL Opportunity Type',\n",
        "    'District',\n",
        "    'Event Title',\n",
        "    'School or Program Site'\n",
        "]\n",
        "\n",
        "# Create temporary column for numeric sum\n",
        "dfmain['_numeric_sum'] = dfmain[numeric_cols].sum(axis=1)\n",
        "\n",
        "# Find duplicates and decide which to keep\n",
        "rows_to_drop = []\n",
        "\n",
        "for idx in dfmain.index:\n",
        "    row = dfmain.loc[idx]\n",
        "\n",
        "    # Look for matches in earlier rows\n",
        "    matches = dfmain[\n",
        "        (dfmain.index < idx) &\n",
        "        (dfmain['Business Champion Name'] == row['Business Champion Name']) &\n",
        "        (dfmain['Student Sponsor Name'] == row['Student Sponsor Name']) &\n",
        "        (dfmain['WBL Opportunity Type'] == row['WBL Opportunity Type']) &\n",
        "        (dfmain['District'] == row['District']) &\n",
        "        (dfmain['Event Title'] == row['Event Title']) &\n",
        "        (dfmain['School or Program Site'] == row['School or Program Site']) &\n",
        "        (dfmain['_numeric_sum'] == row['_numeric_sum']) &\n",
        "        (\n",
        "            (dfmain['Initiation Date'] == row['Initiation Date']) |\n",
        "            (dfmain['Derived Event Date'] == row['Derived Event Date'])\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    if len(matches) > 0:\n",
        "        orig_idx = matches.index[0]\n",
        "        orig_status = dfmain.loc[orig_idx, 'Placement Status']\n",
        "        dup_status = row['Placement Status']\n",
        "\n",
        "        # Rule 1: Prefer 'Completed' status\n",
        "        if orig_status == 'Completed' and dup_status != 'Completed':\n",
        "            rows_to_drop.append(idx)\n",
        "        elif dup_status == 'Completed' and orig_status != 'Completed':\n",
        "            rows_to_drop.append(orig_idx)\n",
        "        else:\n",
        "            # Rule 2: Choose the one with least nulls\n",
        "            orig_null_count = dfmain.loc[orig_idx].isna().sum()\n",
        "            dup_null_count = row.isna().sum()\n",
        "\n",
        "            if orig_null_count < dup_null_count:\n",
        "                rows_to_drop.append(idx)\n",
        "            elif dup_null_count < orig_null_count:\n",
        "                rows_to_drop.append(orig_idx)\n",
        "            else:\n",
        "                # Rule 3: Keep the most recent based on Derived Event Date\n",
        "                orig_date = dfmain.loc[orig_idx, 'Derived Event Date']\n",
        "                dup_date = row['Derived Event Date']\n",
        "\n",
        "                if pd.isna(orig_date) and pd.isna(dup_date):\n",
        "                    rows_to_drop.append(idx)\n",
        "                elif pd.isna(orig_date):\n",
        "                    rows_to_drop.append(orig_idx)\n",
        "                elif pd.isna(dup_date):\n",
        "                    rows_to_drop.append(idx)\n",
        "                elif dup_date > orig_date:\n",
        "                    rows_to_drop.append(orig_idx)\n",
        "                else:\n",
        "                    rows_to_drop.append(idx)\n",
        "\n",
        "# Remove duplicates from rows_to_drop list\n",
        "rows_to_drop = list(set(rows_to_drop))\n",
        "\n",
        "# Drop duplicates\n",
        "dfmain = dfmain.drop(rows_to_drop)\n",
        "\n",
        "# Drop temporary numeric sum column\n",
        "dfmain = dfmain.drop(columns=['_numeric_sum'])"
      ],
      "metadata": {
        "id": "8KCLK8XSiuAn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix 2019-2020 Internship rows marked as Declined that should be Completed\n",
        "declined_should_be_completed = dfmain[\n",
        "    (dfmain['Placement Status Category'] == 'Declined') &\n",
        "    (dfmain['School Year'] == '2019-2020') &\n",
        "    (dfmain['WBL Opportunity Type'].isin(['Internship 60', 'Internship 120', 'Internship 320'])) &\n",
        "    (dfmain['Complete Student Interactions'] > 0)\n",
        "].index\n",
        "\n",
        "dfmain.loc[declined_should_be_completed, 'Placement Status'] = 'Completed'\n",
        "dfmain.loc[declined_should_be_completed, 'Placement Status Category'] = 'Completed'\n",
        "dfmain.loc[declined_should_be_completed, 'Complete Student Internships'] = dfmain.loc[declined_should_be_completed, 'Complete Student Interactions']\n",
        "dfmain.loc[declined_should_be_completed, 'Complete Student Interactions'] = 0"
      ],
      "metadata": {
        "id": "BEvMnzcSTpJv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mark 'Internship In Process' 2024-2025 rows as 'Completed'\n",
        "internship_2024_25 = (\n",
        "    (dfmain['Placement Status'] == 'Internship In Process') &\n",
        "    (dfmain['School Year'] == '2024-2025')\n",
        ")\n",
        "dfmain.loc[internship_2024_25, 'Placement Status'] = 'Completed'\n",
        "\n",
        "# Transfer internship counts to Complete Student Internships for these rows\n",
        "dfmain.loc[internship_2024_25, 'Complete Student Internships'] = dfmain.loc[internship_2024_25, 'Internships in Progress']\n",
        "dfmain.loc[internship_2024_25, 'Internships in Progress'] = 0\n",
        "\n",
        "# Reclassify remaining pending statuses as \"Declined - Unfinished\"\n",
        "pending_statuses = [\n",
        "    'Initial Contact Made', 'Pending-Scheduling', 'Scheduled Interview',\n",
        "    'Internship In Process', 'Scheduled Event (Pending Completion)'\n",
        "]\n",
        "dfmain.loc[dfmain['Placement Status'].isin(pending_statuses), 'Placement Status'] = 'Declined - Unfinished'\n",
        "\n",
        "# Transfer pending counts to declined counts for reclassified rows\n",
        "dfmain.loc[\n",
        "    dfmain['Placement Status'] == 'Declined - Unfinished',\n",
        "    'Declined Student Interactions'\n",
        "] = (\n",
        "    dfmain.loc[dfmain['Placement Status'] == 'Declined - Unfinished', 'Pending Student Interactions'] +\n",
        "    dfmain.loc[dfmain['Placement Status'] == 'Declined - Unfinished', 'Internships in Progress']\n",
        ")\n",
        "\n",
        "# Zero out the pending columns for these rows\n",
        "dfmain.loc[dfmain['Placement Status'] == 'Declined - Unfinished', 'Pending Student Interactions'] = 0\n",
        "dfmain.loc[dfmain['Placement Status'] == 'Declined - Unfinished', 'Internships in Progress'] = 0\n",
        "\n",
        "# Drop the now-irrelevant pending columns\n",
        "dfmain = dfmain.drop(columns=['Pending Student Interactions', 'Internships in Progress'])\n",
        "\n",
        "# Match Placement Status Category with new Placement Status: 'Declined - Unfinished'\n",
        "dfmain['Placement Status Category'] = dfmain['Placement Status'].apply(categorize_placement_status)\n",
        "\n",
        "# Verify\n",
        "print(f\"Rows dropped as duplicates: {len(rows_to_drop)}\")\n",
        "print(f\"Final row count: {len(dfmain)}\")\n",
        "print(f\"\\nPlacement Status counts:\\n\")\n",
        "print(dfmain['Placement Status Category'].value_counts(), \"\\n\")\n",
        "print(dfmain['Placement Status'].value_counts())"
      ],
      "metadata": {
        "id": "Vm-68dYPDLHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5d584a-b30b-4341-f477-b1109957af1f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows dropped as duplicates: 205\n",
            "Final row count: 9922\n",
            "\n",
            "Placement Status counts:\n",
            "\n",
            "Placement Status Category\n",
            "Completed    7651\n",
            "Declined     2190\n",
            "Cancelled      81\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Placement Status\n",
            "Completed                           7651\n",
            "Declined-Applicant                   475\n",
            "Declined - Business Unresponsive     225\n",
            "Declined/Cancelled-Other             224\n",
            "Declined-Business Scheduling         220\n",
            "Declined-Business                    210\n",
            "Declined- Student Applicant          181\n",
            "Declined - Student Profile           176\n",
            "Declined - Student Other             113\n",
            "Declined - Student Unresponsive       90\n",
            "Declined - Staff Scheduling           76\n",
            "Declined-Intern NOT Selected          66\n",
            "Cancelled-COVID                       63\n",
            "Declined - Staff Applicant            35\n",
            "Declined - Unfinished                 35\n",
            "Declined - Staff Unresponsive         26\n",
            "Declined-Opportunity FULL             24\n",
            "Cancelled-Weather                     17\n",
            "Terminated                            14\n",
            "Cancelled-Illness                      1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardizing District Names and Reorganize Column Structure"
      ],
      "metadata": {
        "id": "suD1pQZpQGew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize District Names\n",
        "\n",
        "district_mapping = {\n",
        "    'D11': 'Colorado Springs (D11)',\n",
        "    'D20': 'Academy (D20)',\n",
        "    'D49': 'El Paso County (D49)',\n",
        "    'D2': 'Harrison (D2)',\n",
        "    'D3': 'Widefield (D3)',\n",
        "    'D8': 'Fountain-Fort Carson (D8)',\n",
        "    'D12': 'Cheyenne Mountain (D12)',\n",
        "    'D14': 'Manitou Springs (D14)',\n",
        "    'D38': 'Lewis-Palmer (D38)',\n",
        "    'CEC-CS': 'Colorado Springs Early College (CEC-CS)',\n",
        "    'BLR': 'Banning Lewis Ranch (BLR)',\n",
        "    'WPSD': 'Woodland Park (WPSD)',\n",
        "    'Ellicott': 'Ellicott (D22)',\n",
        "    'Peyton': 'Peyton (D23JT)',\n",
        "    'Calhan': 'Calhan (RJ-1)',\n",
        "    'Atlas Prep': 'Atlas Preparatory',\n",
        "    'CPA/PPOS': 'CO Digital BOCES (CPA/PPOS)',\n",
        "    'PTEC': 'Power Technical (PTEC)',\n",
        "    'Goal H.S.': 'Goal High School',\n",
        "    'Mon Impact': 'Monumental Impact',\n",
        "    'Peak Ed': 'Peak Education',\n",
        "    'Miami-Yoder': 'Miami-Yoder (JT-60)',\n",
        "    'ECA': 'Evangel Christian Academy',\n",
        "    'MET': 'Mountain Employment Training',\n",
        "    'TCA': 'The Classical Academy',\n",
        "    'CCV': 'Cripple Creek-Victor (RE-1)',\n",
        "    'DYS': 'Division of Youth Services',\n",
        "    'Vanguard': 'Vanguard School',\n",
        "    'Homeschool': 'Homeschool',\n",
        "    'Various': 'Various',\n",
        "    'BBBS': 'Big Brothers Big Sisters',\n",
        "    'Roundup': 'Roundup School'\n",
        "}\n",
        "\n",
        "dfmain['District'] = dfmain['District'].replace(district_mapping)\n",
        "\n",
        "# Reorganize Column Structure\n",
        "\n",
        "column_order = [\n",
        "    'School Year',\n",
        "    'Derived Event Date',\n",
        "    'Placement Status Category',\n",
        "    'Placement Status',\n",
        "    'WBL Opportunity Type',\n",
        "    'Event Title',\n",
        "    'Business Champion Name',\n",
        "    'Student Sponsor Name',\n",
        "    'District',\n",
        "    'School or Program Site',\n",
        "    'Complete Student Interactions',\n",
        "    'Complete Student Trainings',\n",
        "    'Complete Student Internships',\n",
        "    'Declined Student Interactions',\n",
        "    'Cancelled Student Interactions',\n",
        "    'Complete Staff Trainings',\n",
        "    'Initiation Date',\n",
        "    'Status Update Date',\n",
        "    'Event Date or Start Date',\n",
        "]\n",
        "\n",
        "dfmain = dfmain[column_order]"
      ],
      "metadata": {
        "id": "l3z7dB8YQJo6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Data Summary"
      ],
      "metadata": {
        "id": "DdY40u6EPixE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"FINAL DATASET SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Size\n",
        "print(f\"\\nSize: {dfmain.shape[0]:,} rows × {dfmain.shape[1]} columns\")\n",
        "\n",
        "# Date Range\n",
        "print(f\"\\nDate Range: {dfmain['Derived Event Date'].min().strftime('%B %d, %Y')} - {dfmain['Derived Event Date'].max().strftime('%B %d, %Y')}\")\n",
        "\n",
        "# Records by Year\n",
        "print(\"\\nRecords by Year:\")\n",
        "year_counts = dfmain['School Year'].value_counts().sort_index()\n",
        "for year, count in year_counts.items():\n",
        "    print(f\"  {year}: {count:,}\")\n",
        "\n",
        "# Status Distribution (by Category)\n",
        "print(\"\\nStatus Distribution (by Category):\")\n",
        "status_counts = dfmain['Placement Status Category'].value_counts()\n",
        "for status, count in status_counts.items():\n",
        "    pct = count / len(dfmain) * 100\n",
        "    print(f\"  {status}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# Status Distribution (detailed)\n",
        "print(\"\\nStatus Distribution (detailed):\")\n",
        "detailed_status = dfmain['Placement Status'].value_counts()\n",
        "for status, count in detailed_status.items():\n",
        "    pct = count / len(dfmain) * 100\n",
        "    print(f\"  {status}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# Student Engagement Metrics\n",
        "print(\"\\nStudent Engagement Metrics:\")\n",
        "total_completed = dfmain['Complete Student Interactions'].sum() + dfmain['Complete Student Trainings'].sum() + dfmain['Complete Student Internships'].sum()\n",
        "total_declined = dfmain['Declined Student Interactions'].sum()\n",
        "total_cancelled = dfmain['Cancelled Student Interactions'].sum()\n",
        "grand_total = total_completed + total_declined + total_cancelled\n",
        "\n",
        "print(f\"  Total Completed: {total_completed:,.0f}\")\n",
        "print(f\"  Total Declined: {total_declined:,.0f}\")\n",
        "print(f\"  Total Cancelled: {total_cancelled:,.0f}\")\n",
        "print(f\"  Grand Total: {grand_total:,.0f} student engagements\")\n",
        "\n",
        "# Average students per completed event\n",
        "completed_events = dfmain[dfmain['Placement Status Category'] == 'Completed']\n",
        "avg_students = total_completed / len(completed_events) if len(completed_events) > 0 else 0\n",
        "print(f\"  Average Students per Completed Event: {avg_students:.1f}\")\n",
        "\n",
        "# Staff Metrics\n",
        "print(\"\\nStaff Metrics:\")\n",
        "print(f\"  Staff Trainings: {dfmain['Complete Staff Trainings'].sum():,.0f}\")\n",
        "\n",
        "# Top 5 Districts\n",
        "print(\"\\nTop 5 Districts:\")\n",
        "top_districts = dfmain['District'].value_counts().head(5)\n",
        "for i, (district, count) in enumerate(top_districts.items(), 1):\n",
        "    print(f\"  {i}. {district}: {count:,} events\")\n",
        "\n",
        "# Top 5 WBL Types\n",
        "print(\"\\nTop 5 WBL Opportunity Types:\")\n",
        "top_wbl = dfmain['WBL Opportunity Type'].value_counts().head(5)\n",
        "for i, (wbl, count) in enumerate(top_wbl.items(), 1):\n",
        "    print(f\"  {i}. {wbl}: {count:,} events\")\n",
        "\n",
        "# Top 5 Businesses\n",
        "print(\"\\nTop 5 Business Partners:\")\n",
        "top_business = dfmain['Business Champion Name'].value_counts().head(5)\n",
        "for i, (business, count) in enumerate(top_business.items(), 1):\n",
        "    print(f\"  {i}. {business}: {count:,} events\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lg_wC-ePig4",
        "outputId": "07c489e0-8a98-4f95-e2fb-3030e7add358"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "FINAL DATASET SUMMARY\n",
            "============================================================\n",
            "\n",
            "Size: 9,922 rows × 19 columns\n",
            "\n",
            "Date Range: January 07, 2019 - September 22, 2025\n",
            "\n",
            "Records by Year:\n",
            "  2019-2020: 330\n",
            "  2020-2021: 1,905\n",
            "  2021-2022: 1,997\n",
            "  2022-2023: 2,743\n",
            "  2023-2024: 1,502\n",
            "  2024-2025: 1,445\n",
            "\n",
            "Status Distribution (by Category):\n",
            "  Completed: 7,651 (77.1%)\n",
            "  Declined: 2,190 (22.1%)\n",
            "  Cancelled: 81 (0.8%)\n",
            "\n",
            "Status Distribution (detailed):\n",
            "  Completed: 7,651 (77.1%)\n",
            "  Declined-Applicant: 475 (4.8%)\n",
            "  Declined - Business Unresponsive: 225 (2.3%)\n",
            "  Declined/Cancelled-Other: 224 (2.3%)\n",
            "  Declined-Business Scheduling: 220 (2.2%)\n",
            "  Declined-Business: 210 (2.1%)\n",
            "  Declined- Student Applicant: 181 (1.8%)\n",
            "  Declined - Student Profile: 176 (1.8%)\n",
            "  Declined - Student Other: 113 (1.1%)\n",
            "  Declined - Student Unresponsive: 90 (0.9%)\n",
            "  Declined - Staff Scheduling: 76 (0.8%)\n",
            "  Declined-Intern NOT Selected: 66 (0.7%)\n",
            "  Cancelled-COVID: 63 (0.6%)\n",
            "  Declined - Staff Applicant: 35 (0.4%)\n",
            "  Declined - Unfinished: 35 (0.4%)\n",
            "  Declined - Staff Unresponsive: 26 (0.3%)\n",
            "  Declined-Opportunity FULL: 24 (0.2%)\n",
            "  Cancelled-Weather: 17 (0.2%)\n",
            "  Terminated: 14 (0.1%)\n",
            "  Cancelled-Illness: 1 (0.0%)\n",
            "\n",
            "Student Engagement Metrics:\n",
            "  Total Completed: 52,350\n",
            "  Total Declined: 8,771\n",
            "  Total Cancelled: 1,764\n",
            "  Grand Total: 62,885 student engagements\n",
            "  Average Students per Completed Event: 6.8\n",
            "\n",
            "Staff Metrics:\n",
            "  Staff Trainings: 1,382\n",
            "\n",
            "Top 5 Districts:\n",
            "  1. Colorado Springs (D11): 3,246 events\n",
            "  2. Academy (D20): 1,316 events\n",
            "  3. El Paso County (D49): 1,302 events\n",
            "  4. Fountain-Fort Carson (D8): 759 events\n",
            "  5. Widefield (D3): 711 events\n",
            "\n",
            "Top 5 WBL Opportunity Types:\n",
            "  1. Professionalism 101 Training: 3,807 events\n",
            "  2. e-WBL Informational Interview: 1,498 events\n",
            "  3. Job Shadow: 1,167 events\n",
            "  4. Internship 60: 964 events\n",
            "  5. Speakers Bureau: 755 events\n",
            "\n",
            "Top 5 Business Partners:\n",
            "  1. PPBEA: 4,214 events\n",
            "  2. UCCS College of Nursing & Health Sciences: 308 events\n",
            "  3. Children’s Hospital Colorado, Colorado Springs: 220 events\n",
            "  4. Jaxon Engineering & Maintenance: 180 events\n",
            "  5. D49 - El Paso County School District 49: 125 events\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export"
      ],
      "metadata": {
        "id": "Ztcgt71APodE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "output_path = '/content/drive/MyDrive/Work/BEA/2025 BEA Data Project Shared Folder/Data/(Main) Data Sources/Existing/PPBEA Pipeline/Cleaned/'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "output_file = f'{output_path}PPBEA_Pipeline_2019-2025_Cleaned.csv'\n",
        "dfmain.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Cleaned data saved to: {output_file}\")\n",
        "print(f\"Final shape: {dfmain.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SLRQHJzPs7E",
        "outputId": "5135100c-5c99-4494-94ff-e8876f88e7a6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to: /content/drive/MyDrive/Work/BEA/2025 BEA Data Project Shared Folder/Data/(Main) Data Sources/Existing/PPBEA Pipeline/Cleaned/PPBEA_Pipeline_2019-2025_Cleaned.csv\n",
            "Final shape: (9922, 19)\n"
          ]
        }
      ]
    }
  ]
}