{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONddVZK00dbcgsP2Qs1d0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caseyeaston/BEA_PipelineEngagementAnalysis/blob/main/BEA_PipelineEngagementCleaningFinal_Logging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "n9Wy7FDaG6qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LKl1obNgsqeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe31499-9bee-4c3c-ca84-4a06ec719051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', None)"
      ],
      "metadata": {
        "id": "1njSdg8Pss0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Change Tracking"
      ],
      "metadata": {
        "id": "h4cyWqaHI58Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create changes log dataframe\n",
        "changes_log = pd.DataFrame()\n",
        "current_log_id = 0\n",
        "\n",
        "def log_dropped(rows_df, reason, description):\n",
        "    \"\"\"\n",
        "    Log rows that are being dropped (no original needed - these ARE the originals).\n",
        "    Each row gets its own unique _log_id.\n",
        "    \"\"\"\n",
        "    global changes_log, current_log_id\n",
        "    if len(rows_df) > 0:\n",
        "        log_entry = rows_df.copy()\n",
        "        log_entry['_change_reason_description'] = f\"{reason} - {description}\"\n",
        "        # log_entry['_change_type'] = 'dropped'\n",
        "        log_entry['_row_version'] = 'dropped'\n",
        "        # Each dropped row gets its own unique ID\n",
        "        log_entry['_log_id'] = range(current_log_id, current_log_id + len(rows_df))\n",
        "        current_log_id += len(rows_df)\n",
        "        changes_log = pd.concat([changes_log, log_entry], ignore_index=True)\n",
        "        print(f\"  Logged {len(rows_df)} dropped rows: {reason}\")\n",
        "\n",
        "def log_original_only(rows_df, reason, description):\n",
        "    \"\"\"\n",
        "    Log only the original rows (for changes where we don't need to show the modified version).\n",
        "    Each row gets its own unique _log_id.\n",
        "    \"\"\"\n",
        "    global changes_log, current_log_id\n",
        "    if len(rows_df) > 0:\n",
        "        log_entry = rows_df.copy()\n",
        "        log_entry['_change_reason_description'] = f\"{reason} - {description}\"\n",
        "        # log_entry['_change_type'] = 'modified'\n",
        "        log_entry['_row_version'] = 'original'\n",
        "        # Each row gets its own unique ID\n",
        "        log_entry['_log_id'] = range(current_log_id, current_log_id + len(rows_df))\n",
        "        current_log_id += len(rows_df)\n",
        "        changes_log = pd.concat([changes_log, log_entry], ignore_index=True)\n",
        "        print(f\"  Logged {len(rows_df)} original rows: {reason}\")\n",
        "\n",
        "def log_duplicates(kept_rows_df, dropped_rows_df, reason, description):\n",
        "    \"\"\"\n",
        "    Log duplicate rows - includes the kept row AND the dropped duplicates.\n",
        "    The kept row and its duplicates share the same _log_id.\n",
        "    \"\"\"\n",
        "    global changes_log, current_log_id\n",
        "    if len(dropped_rows_df) > 0:\n",
        "        # For each kept row, assign an ID that its duplicates will share\n",
        "        kept_entry = kept_rows_df.copy()\n",
        "        kept_entry['_change_reason_description'] = f\"{reason} - {description}\"\n",
        "        # kept_entry['_change_type'] = 'duplicate_kept'\n",
        "        kept_entry['_row_version'] = 'kept'\n",
        "        kept_entry['_log_id'] = range(current_log_id, current_log_id + len(kept_rows_df))\n",
        "\n",
        "        # Dropped duplicates get the same ID as their kept counterpart\n",
        "        dropped_entry = dropped_rows_df.copy()\n",
        "        dropped_entry['_change_reason_description'] = f\"{reason} - {description}\"\n",
        "        # dropped_entry['_change_type'] = 'duplicate_dropped'\n",
        "        dropped_entry['_row_version'] = 'dropped'\n",
        "        dropped_entry['_log_id'] = range(current_log_id, current_log_id + len(dropped_rows_df))\n",
        "\n",
        "        current_log_id += max(len(kept_rows_df), len(dropped_rows_df))\n",
        "\n",
        "        changes_log = pd.concat([changes_log, kept_entry, dropped_entry], ignore_index=True)\n",
        "        print(f\"  Logged {len(dropped_rows_df)} dropped duplicates (with {len(kept_rows_df)} kept rows): {reason}\")\n",
        "\n",
        "def log_created(original_rows_df, created_rows_df, reason, description):\n",
        "    \"\"\"\n",
        "    Log newly created rows (e.g., Pro101 splits).\n",
        "    Logs the original row AND the newly created row.\n",
        "    Original and created rows share the same _log_id.\n",
        "    \"\"\"\n",
        "    global changes_log, current_log_id\n",
        "    if len(created_rows_df) > 0:\n",
        "        # Log original rows\n",
        "        orig_entry = original_rows_df.copy()\n",
        "        orig_entry['_change_reason_description'] = f\"{reason} - {description}\"\n",
        "        # orig_entry['_change_type'] = 'split'\n",
        "        orig_entry['_row_version'] = 'original'\n",
        "        orig_entry['_log_id'] = range(current_log_id, current_log_id + len(original_rows_df))\n",
        "\n",
        "        # Log created rows with same IDs as their originals\n",
        "        created_entry = created_rows_df.copy()\n",
        "        created_entry['_change_reason_description'] = f\"{reason} - {description}\"\n",
        "        # created_entry['_change_type'] = 'split'\n",
        "        created_entry['_row_version'] = 'created'\n",
        "        created_entry['_log_id'] = range(current_log_id, current_log_id + len(created_rows_df))\n",
        "\n",
        "        current_log_id += len(original_rows_df)\n",
        "\n",
        "        changes_log = pd.concat([changes_log, orig_entry, created_entry], ignore_index=True)\n",
        "        print(f\"  Logged {len(created_rows_df)} created rows (with {len(original_rows_df)} source rows): {reason}\")\n",
        "\n",
        "print(\"Change tracking initialized.\")"
      ],
      "metadata": {
        "id": "NYMAY2XKI_xL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68fc9127-f526-407d-a8b7-f75c929cac07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change tracking initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "HUm6DC7OJFSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths\n",
        "base_path = '/content/drive/MyDrive/Work/BEA/2025 BEA Data Project Shared Folder/Data/(Main) Data Sources/Existing/PPBEA Pipeline/CSVs/'\n",
        "file_paths = {\n",
        "    '2019-2020': f'{base_path}2019-2020_PPBEA Pipeline_Engagement.csv',\n",
        "    '2020-2021': f'{base_path}2020-2021_PPBEA Pipeline_Engagement.csv',\n",
        "    '2021-2022': f'{base_path}2021-2022_PPBEA Pipeline_Engagement.csv',\n",
        "    '2022-2023': f'{base_path}2022-2023_PPBEA Pipeline_Engagement.csv',\n",
        "    '2023-2024': f'{base_path}2023-2024_PPBEA Pipeline_Engagement.csv',\n",
        "    '2024-2025': f'{base_path}2024-2025_PPBEA Pipeline_Engagement.csv',\n",
        "}\n",
        "\n",
        "# Load all CSV files\n",
        "dfs = {}\n",
        "for year, path in file_paths.items():\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.dropna(how='all')  # Remove completely empty rows\n",
        "    df['School Year'] = year  # Add school year identifier\n",
        "    dfs[year] = df\n",
        "\n",
        "# Combine all dataframes\n",
        "dfmain = pd.concat(dfs.values(), ignore_index=True)"
      ],
      "metadata": {
        "id": "qku6mOr_tMZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename PPBEA Member to District\n",
        "dfmain = dfmain.rename(columns={'PPBEA Member': 'District'})\n",
        "\n",
        "# Drop district columns and other unwanted columns\n",
        "columns_to_drop = [\n",
        "    ' ',  # Unnamed column\n",
        "    'Calhan District RJ-1', 'Harrison District 2', 'Widefield District 3',\n",
        "    'Fountain Ft.Carson District 8', 'Colorado Springs District 11',\n",
        "    'Cheyenne Mountain District 12', 'Manitou Springs District 14',\n",
        "    'Academy District 20', 'Ellicott District 22', 'Peyton District 23JT',\n",
        "    'Lewis Palmer District 38', 'El Paso County District 49',\n",
        "    'Colorado Springs Early College (CSEC)', 'CO Digital BOCES PPOS & CPA',\n",
        "    'Eastlake High School', 'Banning Lewis Ranch', 'Atlas Prep',\n",
        "    'Woodland Park School District',\n",
        "    'Unnamed: 24',\n",
        "    'Career Rep Email', 'Follow-up Task: ', 'Employer post Internship',\n",
        "    'Sponsor Email', 'Placed into Employment Post Internship',\n",
        "    'Staff Interactions with Businesses', 'Career Rep First Name',\n",
        "    'Career Rep Last Name', 'Opp Number', 'Task Number',\n",
        "    'PPBEA Staff Assigned', 'Next Action'\n",
        "]\n",
        "dfmain = dfmain.drop(columns=columns_to_drop)"
      ],
      "metadata": {
        "id": "cc03uaaDtOPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge duplicate columns\n",
        "dfmain['Pro101 Certificates Earned'] = dfmain['Pro101 Certificates Earned'].fillna(\n",
        "    dfmain['Professionalism 101 Certificates Earned']\n",
        ")\n",
        "dfmain = dfmain.drop(columns=[\n",
        "    'Professionalism 101 Certificates Earned',\n",
        "])\n",
        "\n",
        "dfmain['PPBEA Notes'] = dfmain['PPBEA Notes'].fillna(\n",
        "    dfmain['Notes: Student Name, Duration, School Name, Sponsor Name, Teacher Name, Flags']\n",
        ")\n",
        "dfmain = dfmain.drop(columns=[\n",
        "    'Notes: Student Name, Duration, School Name, Sponsor Name, Teacher Name, Flags',\n",
        "])"
      ],
      "metadata": {
        "id": "S0IklkYRtQGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numeric columns\n",
        "numeric_cols = [\n",
        "    'Complete Student Trainings',\n",
        "    'Complete Staff Trainings',\n",
        "    'Complete Student Interactions',\n",
        "    'Complete Student Internships',\n",
        "    'Internships in Progress',\n",
        "    'Pending Student Interactions',\n",
        "    'Declined or Cancelled Student Interactions',\n",
        "    'Pro101 Certificates Earned'\n",
        "]\n",
        "\n",
        "for col in numeric_cols:\n",
        "    dfmain[col] = pd.to_numeric(dfmain[col], errors='coerce')\n",
        "\n",
        "# Fill nulls with 0 for numeric columns\n",
        "for col in numeric_cols:\n",
        "    dfmain[col] = dfmain[col].fillna(0)\n",
        "\n",
        "print(f\"Loaded {len(dfmain):,} rows\")"
      ],
      "metadata": {
        "id": "xMsl0xOhtSPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d169340-6301-4d52-df69-070be29c8424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10,165 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Summary and Empty Rows (LOG: dropped rows)"
      ],
      "metadata": {
        "id": "jujfAyX2JrZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify summary and empty rows\n",
        "summary_rows = dfmain[\n",
        "    (dfmain['Complete Student Interactions'] > 2000) |\n",
        "    (dfmain['Event Title'].isna())\n",
        "]\n",
        "\n",
        "# LOG: dropped summary/empty rows\n",
        "log_dropped(summary_rows, \"Summary/empty rows\", \"dropped\")\n",
        "\n",
        "# Remove them\n",
        "dfmain = dfmain[\n",
        "    ((dfmain['Complete Student Interactions'] <= 2000) | (dfmain['Complete Student Interactions'].isna())) &\n",
        "    (dfmain['Event Title'].notna())\n",
        "]\n",
        "\n",
        "# Convert date columns to datetime\n",
        "dfmain['Initiation Date'] = pd.to_datetime(dfmain['Initiation Date'], errors='coerce')\n",
        "dfmain['Status Update Date'] = pd.to_datetime(dfmain['Status Update Date'], errors='coerce')\n",
        "dfmain['Event Date or Start Date'] = pd.to_datetime(dfmain['Event Date or Start Date'], errors='coerce')\n",
        "\n",
        "# Standardize WBL Opportunity Type\n",
        "dfmain['WBL Opportunity Type'] = dfmain['WBL Opportunity Type'].replace({\n",
        "    \"Speaker's Bureau\": \"Speakers Bureau\"\n",
        "})\n",
        "\n",
        "# Clean text columns (strip whitespace/newlines)\n",
        "dfmain = dfmain.copy()\n",
        "dfmain['Business Champion Name'] = dfmain['Business Champion Name'].str.strip()\n",
        "dfmain = dfmain.rename(columns={'Student and Sponsor\\nor School POC Name': 'Student Sponsor Name'})\n",
        "dfmain['Student Sponsor Name'] = dfmain['Student Sponsor Name'].str.strip()\n",
        "\n",
        "print(f\"Rows after removing summary/empty: {len(dfmain):,}\")"
      ],
      "metadata": {
        "id": "2luFKBi_Jx68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806d8347-88d9-4492-d773-e8684605b0d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Logged 96 dropped rows: Summary/empty rows\n",
            "Rows after removing summary/empty: 10,069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Numbers Comparison Before Cleaning"
      ],
      "metadata": {
        "id": "z3mvMiSYJ6Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Company reported numbers (annual)\n",
        "company_reported = {\n",
        "    '2019-2020': 3233,\n",
        "    '2020-2021': 4056,\n",
        "    '2021-2022': 6787,\n",
        "    '2022-2023': 9815,\n",
        "    '2023-2024': 11865,\n",
        "    '2024-2025': 14135\n",
        "}\n",
        "company_total = 49891\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"INITIAL NUMBERS COMPARISON\")\n",
        "print(\"(After removing summary rows, before other cleaning)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nTotal rows: {len(dfmain):,}\")\n",
        "\n",
        "# Define complete columns\n",
        "complete_cols_initial = [\n",
        "    'Complete Student Interactions',\n",
        "    'Complete Student Trainings',\n",
        "    'Complete Student Internships'\n",
        "]\n",
        "\n",
        "# Comparison by School Year (Completed Status Only)\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"COMPARISON BY SCHOOL YEAR (Completed Status Only)\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Year':<15} {'Company':<12} {'Our Data':<12} {'Difference':<12}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "total_company = 0\n",
        "total_ours = 0\n",
        "\n",
        "for year in sorted(dfmain['School Year'].unique()):\n",
        "    year_df = dfmain[(dfmain['School Year'] == year) & (dfmain['Placement Status'] == 'Completed')]\n",
        "\n",
        "    year_total = year_df[complete_cols_initial].sum().sum()\n",
        "    year_total += year_df['Pro101 Certificates Earned'].sum()\n",
        "\n",
        "    company_num = company_reported.get(year, 0)\n",
        "    diff = year_total - company_num\n",
        "\n",
        "    total_company += company_num\n",
        "    total_ours += year_total\n",
        "\n",
        "    print(f\"{year:<15} {company_num:<12,} {year_total:<12,.0f} {diff:<+12,.0f}\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'TOTAL':<15} {total_company:<12,} {total_ours:<12,.0f} {total_ours - total_company:<+12,.0f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ],
      "metadata": {
        "id": "eKwTFH3vKAWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2a8b89-bb4c-4954-c7c0-bb2a53f8324c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "INITIAL NUMBERS COMPARISON\n",
            "(After removing summary rows, before other cleaning)\n",
            "======================================================================\n",
            "\n",
            "Total rows: 10,069\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "COMPARISON BY SCHOOL YEAR (Completed Status Only)\n",
            "----------------------------------------------------------------------\n",
            "Year            Company      Our Data     Difference  \n",
            "----------------------------------------------------------------------\n",
            "2019-2020       3,233        3,905        +672        \n",
            "2020-2021       4,056        3,965        -91         \n",
            "2021-2022       6,787        6,858        +71         \n",
            "2022-2023       9,815        9,825        +10         \n",
            "2023-2024       11,865       12,022       +157        \n",
            "2024-2025       14,135       14,732       +597        \n",
            "----------------------------------------------------------------------\n",
            "TOTAL           49,891       51,307       +1,416      \n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop One-off WBL Type (LOG: dropped row)"
      ],
      "metadata": {
        "id": "MaYqbZd5KYGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the one-off WBL type row\n",
        "bad_wbl_rows = dfmain[dfmain['WBL Opportunity Type'] == 'Jobs/Training/Apprenticeship']\n",
        "\n",
        "# LOG: dropped row\n",
        "log_dropped(bad_wbl_rows, \"One-off WBL type\", \"dropped\")\n",
        "\n",
        "# Drop it\n",
        "dfmain = dfmain[dfmain['WBL Opportunity Type'] != 'Jobs/Training/Apprenticeship']\n",
        "\n",
        "# Clean text columns (strip whitespace/newlines)\n",
        "dfmain = dfmain.copy()\n",
        "dfmain['Business Champion Name'] = dfmain['Business Champion Name'].str.strip()\n",
        "dfmain = dfmain.rename(columns={'Student and Sponsor\\nor School POC Name': 'Student Sponsor Name'})\n",
        "dfmain['Student Sponsor Name'] = dfmain['Student Sponsor Name'].str.strip()\n",
        "\n",
        "# Checking shape\n",
        "dfmain.shape"
      ],
      "metadata": {
        "id": "Cj2uNDmpKV7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402a525c-e701-4ac3-ead9-2fbcf0232009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Logged 1 dropped rows: One-off WBL type\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10068, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Date Errors and Nulls (LOG: original and modified rows)"
      ],
      "metadata": {
        "id": "98vpbrd3IHDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix unrealistic dates (before 2018)\n",
        "date_cols = ['Initiation Date', 'Status Update Date', 'Event Date or Start Date']\n",
        "cutoff_date = pd.Timestamp('2018-01-01')\n",
        "\n",
        "for col in date_cols:\n",
        "    unrealistic_mask = dfmain[col] < cutoff_date\n",
        "    count = unrealistic_mask.sum()\n",
        "    if count > 0:\n",
        "        # LOG: original rows only\n",
        "        original_rows = dfmain[unrealistic_mask].copy()\n",
        "        log_original_only(original_rows, \"Unrealistic dates\", f\"set to null in {col}\")\n",
        "\n",
        "        # Make the modification\n",
        "        dfmain.loc[unrealistic_mask, col] = pd.NaT\n",
        "        print(f\"Setting {count} unrealistic dates to null in {col}\")\n",
        "\n",
        "# Fill remaining nulls in date columns using cascade logic\n",
        "dfmain['Initiation Date'] = dfmain['Initiation Date'].fillna(dfmain['Event Date or Start Date']).fillna(dfmain['Status Update Date'])\n",
        "dfmain['Status Update Date'] = dfmain['Status Update Date'].fillna(dfmain['Event Date or Start Date']).fillna(dfmain['Initiation Date'])\n",
        "\n",
        "# Create Derived Event Date column with fallback logic\n",
        "# Priority: Event Date or Start Date → Status Update Date → Initiation Date\n",
        "dfmain['Derived Event Date'] = dfmain['Event Date or Start Date'].fillna(\n",
        "    dfmain['Status Update Date']\n",
        ").fillna(\n",
        "    dfmain['Initiation Date']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASOzjm7_QCW3",
        "outputId": "d60a4f58-9ce5-4a06-f81b-247cd7c1c874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Logged 4 original rows: Unrealistic dates\n",
            "Setting 4 unrealistic dates to null in Initiation Date\n",
            "  Logged 7 original rows: Unrealistic dates\n",
            "Setting 7 unrealistic dates to null in Status Update Date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 'Placement Status' Parent Column & Separating Declined/Cancelled"
      ],
      "metadata": {
        "id": "mj7s-SHWz9F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create parent category column\n",
        "def categorize_placement_status(status):\n",
        "    if status == 'Completed':\n",
        "        return 'Completed'\n",
        "    elif status in ['Cancelled-COVID', 'Cancelled-Weather', 'Cancelled-Illness']:\n",
        "        return 'Cancelled'\n",
        "    elif status in ['Initial Contact Made', 'Pending-Scheduling', 'Scheduled Interview',\n",
        "    'Internship In Process', 'Scheduled Event (Pending Completion)']:\n",
        "        return 'Pending'\n",
        "    else:\n",
        "        return 'Declined'\n",
        "\n",
        "dfmain['Placement Status Category'] = dfmain['Placement Status'].apply(categorize_placement_status)\n",
        "\n",
        "# Verify\n",
        "dfmain['Placement Status Category'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "v233isnh1PWy",
        "outputId": "9286d062-83ab-438b-9ac5-3a617e1f9031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Placement Status Category\n",
              "Completed    7549\n",
              "Declined     2161\n",
              "Pending       277\n",
              "Cancelled      81\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Placement Status Category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Completed</th>\n",
              "      <td>7549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Declined</th>\n",
              "      <td>2161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pending</th>\n",
              "      <td>277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cancelled</th>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Cancelled column and split Declined/Cancelled\n",
        "dfmain['Cancelled Student Interactions'] = 0\n",
        "dfmain = dfmain.rename(columns={\n",
        "    'Declined or Cancelled Student Interactions': 'Declined Student Interactions'\n",
        "})\n",
        "\n",
        "# Move Cancelled values to correct column based on Placement Status Category\n",
        "cancelled_mask = dfmain['Placement Status Category'] == 'Cancelled'\n",
        "dfmain.loc[cancelled_mask, 'Cancelled Student Interactions'] = dfmain.loc[cancelled_mask, 'Declined Student Interactions']\n",
        "dfmain.loc[cancelled_mask, 'Declined Student Interactions'] = 0\n",
        "\n",
        "# Convert 'Cancelled Student Interactions' to float datatype\n",
        "dfmain['Cancelled Student Interactions'] = dfmain['Cancelled Student Interactions'].astype(float)\n",
        "\n",
        "# Redefine numeric columns\n",
        "numeric_cols = [\n",
        "    'Complete Student Trainings',\n",
        "    'Complete Staff Trainings',\n",
        "    'Complete Student Interactions',\n",
        "    'Complete Student Internships',\n",
        "    'Internships in Progress',\n",
        "    'Pending Student Interactions',\n",
        "    'Declined Student Interactions',\n",
        "    'Cancelled Student Interactions',\n",
        "    'Pro101 Certificates Earned'\n",
        "]"
      ],
      "metadata": {
        "id": "Gu14y8tVPewQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pro101 (LOG: original and created rows)"
      ],
      "metadata": {
        "id": "W5PjFE61sB4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from rapidfuzz import fuzz\n",
        "\n",
        "# # Extract student name (before \"/\") from Student Sponsor Name\n",
        "# def extract_student_name(name):\n",
        "#     if pd.isna(name):\n",
        "#         return ''\n",
        "#     name_str = str(name)\n",
        "#     if '/' in name_str:\n",
        "#         return name_str.split('/')[0].strip()\n",
        "#     else:\n",
        "#         return name_str.strip()\n",
        "\n",
        "# # Add temporary column for student names\n",
        "# dfmain['Student Name'] = dfmain['Student Sponsor Name'].apply(extract_student_name)\n",
        "\n",
        "# # Find rows with Pro101 cert earned during other events\n",
        "# pro101_during_other_event = dfmain[\n",
        "#     (dfmain['Pro101 Certificates Earned'] > 0) &\n",
        "#     (dfmain['WBL Opportunity Type'] != 'Professionalism 101 Training')\n",
        "# ]\n",
        "\n",
        "# print(f\"Rows with Pro101 cert earned during OTHER events: {len(pro101_during_other_event)}\")\n",
        "\n",
        "# # Check if any already have matching standalone Pro101 rows\n",
        "# potential_duplicates = []\n",
        "\n",
        "# for idx, row in pro101_during_other_event.iterrows():\n",
        "#     # Get student name from this row\n",
        "#     student_name = row['Student Name']\n",
        "\n",
        "#     if not student_name:  # Skip if no student name\n",
        "#         continue\n",
        "\n",
        "#     # Look for Pro101 Training rows with fuzzy match on student name\n",
        "#     pro101_rows = dfmain[dfmain['WBL Opportunity Type'] == 'Professionalism 101 Training']\n",
        "\n",
        "#     for pro101_idx, pro101_row in pro101_rows.iterrows():\n",
        "#         pro101_student_name = pro101_row['Student Name']\n",
        "\n",
        "#         if not pro101_student_name:\n",
        "#             continue\n",
        "\n",
        "#         # Fuzzy match on student names\n",
        "#         similarity = fuzz.ratio(student_name.lower(), pro101_student_name.lower())\n",
        "\n",
        "#         if similarity >= 85:  # 85% threshold\n",
        "#             potential_duplicates.append((idx, pro101_idx, similarity))\n",
        "#             break  # Found a match, move to next row\n",
        "\n",
        "# print(f\"\\nRows that already have standalone Pro101 records: {len(potential_duplicates)}\")\n",
        "\n",
        "# if len(potential_duplicates) > 0:\n",
        "#     print(\"\\nSample matches (first 10):\")\n",
        "#     for orig_idx, pro101_idx, similarity in potential_duplicates[:10]:\n",
        "#         print(f\"\\nOriginal event row {orig_idx} matches Pro101 row {pro101_idx} (similarity: {similarity}%)\")\n",
        "#         print(f\"  Original: {dfmain.loc[orig_idx, 'Student Name']} - {dfmain.loc[orig_idx, 'WBL Opportunity Type']}\")\n",
        "#         print(f\"  Pro101:   {dfmain.loc[pro101_idx, 'Student Name']} - {dfmain.loc[pro101_idx, 'WBL Opportunity Type']}\")"
      ],
      "metadata": {
        "id": "Biud2DCf75_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 2 existing Pro101 rows that we'll recreate from their matching events\n",
        "dfmain = dfmain.drop([1098, 540])\n",
        "\n",
        "# Find rows that need to be split (Pro101 earned during another event)\n",
        "rows_to_split = dfmain[\n",
        "    (dfmain['Pro101 Certificates Earned'] > 0) &\n",
        "    (dfmain['WBL Opportunity Type'] != 'Professionalism 101 Training')\n",
        "].copy()\n",
        "\n",
        "# Create new Pro101 rows\n",
        "new_pro101_rows = []\n",
        "\n",
        "for idx, row in rows_to_split.iterrows():\n",
        "    pro101_row = row.copy()\n",
        "    pro101_row['Placement Status'] = 'Completed'\n",
        "    pro101_row['Placement Status Category'] = 'Completed'\n",
        "    pro101_row['Business Champion Name'] = 'PPBEA'\n",
        "    pro101_row['Event Title'] = 'PPBEA Professionalism 101 Course'\n",
        "    pro101_row['WBL Opportunity Type'] = 'Professionalism 101 Training'\n",
        "    pro101_row['Complete Student Interactions'] = 1\n",
        "    pro101_row['Complete Student Trainings'] = 0\n",
        "    pro101_row['Complete Staff Trainings'] = 0\n",
        "    pro101_row['Complete Student Internships'] = 0\n",
        "    pro101_row['Internships in Progress'] = 0\n",
        "    pro101_row['Pending Student Interactions'] = 0\n",
        "    pro101_row['Declined Student Interactions'] = 0\n",
        "    pro101_row['Cancelled Student Interactions'] = 0\n",
        "    pro101_row['Pro101 Certificates Earned'] = 0\n",
        "    new_pro101_rows.append(pro101_row)\n",
        "\n",
        "# LOG: original rows and created Pro101 rows\n",
        "if len(new_pro101_rows) > 0:\n",
        "    created_df = pd.DataFrame(new_pro101_rows)\n",
        "    log_created(rows_to_split, created_df, \"Separate Pro101 Cert from non-Pro101 event\", \"split into original event and new Pro101 row\")\n",
        "\n",
        "# Add new Pro101 rows to dfmain\n",
        "dfmain = pd.concat([dfmain, pd.DataFrame(new_pro101_rows)], ignore_index=True)\n",
        "\n",
        "# Transfer 'Pro101 Certificates Earned' to 'Complete Student Interactions'\n",
        "pro101_completed_wrong = dfmain[\n",
        "    (dfmain['Placement Status'] == 'Completed') &\n",
        "    (dfmain['WBL Opportunity Type'] == 'Professionalism 101 Training') &\n",
        "    (dfmain['Complete Student Interactions'] == 0)\n",
        "]\n",
        "\n",
        "dfmain.loc[pro101_completed_wrong.index, 'Complete Student Interactions'] = 1\n",
        "\n",
        "# Drop Pro101 column (no longer needed)\n",
        "dfmain = dfmain.drop(columns=['Pro101 Certificates Earned'])\n",
        "\n",
        "print(f\"Created {len(new_pro101_rows)} new Pro101 rows\")\n",
        "dfmain.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbq3upO8KKFN",
        "outputId": "c411e207-65ee-4bff-ceb5-dfb867375410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Logged 61 created rows (with 61 source rows): Separate Pro101 Cert from non-Pro101 event\n",
            "Created 61 new Pro101 rows\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10127, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Placement Status & Numeric Columns Mismatch"
      ],
      "metadata": {
        "id": "Fc6r5G58MOf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fix Double-Counting (LOG: original and modified rows)"
      ],
      "metadata": {
        "id": "nrnxc784LCyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefine numeric columns\n",
        "numeric_cols = [\n",
        "    'Complete Student Trainings',\n",
        "    'Complete Staff Trainings',\n",
        "    'Complete Student Interactions',\n",
        "    'Complete Student Internships',\n",
        "    'Internships in Progress',\n",
        "    'Pending Student Interactions',\n",
        "    'Declined Student Interactions',\n",
        "    'Cancelled Student Interactions',\n",
        "]\n",
        "\n",
        "# Fix double-counting: rows with values in multiple numeric columns\n",
        "dfmain['num_cols_with_values'] = (dfmain[numeric_cols] > 0).sum(axis=1)\n",
        "rows_with_multiple_mask = dfmain['num_cols_with_values'] > 1\n",
        "\n",
        "if rows_with_multiple_mask.sum() > 0:\n",
        "    # LOG: original rows only\n",
        "    original_rows = dfmain[rows_with_multiple_mask].copy()\n",
        "    log_original_only(original_rows, \"Fix student trainings and interactions double-counting\", \"zeroed 'Complete Student Interactions'\")\n",
        "\n",
        "# Zero out Complete Student Interactions for rows with Trainings + Interactions\n",
        "dfmain.loc[rows_with_multiple_mask, 'Complete Student Interactions'] = 0\n",
        "\n",
        "# Drop helper column\n",
        "dfmain = dfmain.drop(columns=['num_cols_with_values'])\n",
        "\n",
        "# Create a column for numeric sum\n",
        "dfmain['_numeric_sum'] = dfmain[numeric_cols].sum(axis=1)"
      ],
      "metadata": {
        "id": "KjXqWJ11LFEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2339b92c-8778-445a-9303-766aa726f5bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Logged 24 original rows: Fix student trainings and interactions double-counting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fill All-Zero Rows (LOG: original and modified rows)"
      ],
      "metadata": {
        "id": "TkzfvSGz4dGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all-zero rows\n",
        "all_zeros = (dfmain[numeric_cols] == 0).all(axis=1)\n",
        "\n",
        "# LOG: original all-zero rows - ONLY COMPLETED STATUS\n",
        "all_zeros_completed = all_zeros & (dfmain['Placement Status Category'] == 'Completed')\n",
        "if all_zeros_completed.sum() > 0:\n",
        "    original_all_zero_rows = dfmain[all_zeros_completed].copy()\n",
        "    log_original_only(original_all_zero_rows, \"All-zero rows\", \"filled with min value based on WBL type\")\n",
        "\n",
        "# Calculate minimum numeric sum by WBL Opportunity Type (excluding zeros)\n",
        "wbl_minimums = dfmain[dfmain['_numeric_sum'] > 0].groupby('WBL Opportunity Type')['_numeric_sum'].min()\n",
        "\n",
        "# Get WBL types that have all-zero rows\n",
        "wbl_types_with_zeros = dfmain[all_zeros]['WBL Opportunity Type'].unique()\n",
        "\n",
        "print(\"Minimum values by WBL Opportunity Type (for types with all-zero rows):\")\n",
        "for wbl_type in sorted(wbl_types_with_zeros):\n",
        "    if pd.notna(wbl_type):\n",
        "        min_val = wbl_minimums.get(wbl_type, 1)\n",
        "        print(f\"  {wbl_type}: {min_val}\")\n",
        "\n",
        "# Show value counts of WBL Opportunity Type for all-zero rows\n",
        "print(\"\\nWBL Opportunity Type breakdown for all-zero rows:\")\n",
        "print(dfmain[all_zeros]['WBL Opportunity Type'].value_counts())\n",
        "print(f\"\\nTotal all-zero rows: {all_zeros.sum()}\")\n",
        "\n",
        "# WBL Opportunity Type to numeric column mapping (Completed)\n",
        "wbl_to_column_map_completed = {\n",
        "    'Staff Training': 'Complete Staff Trainings',\n",
        "    'Regional Advisory Meeting': 'Complete Staff Trainings',\n",
        "    'Site Visit - Staff': 'Complete Staff Trainings',\n",
        "    'Student Training': 'Complete Student Trainings',\n",
        "    'Professionalism 101 Training': 'Complete Student Interactions',\n",
        "    'Informational Interview Video': 'Complete Student Interactions',\n",
        "    'Career Story Video': 'Complete Student Interactions',\n",
        "    'e-WBL Informational Interview': 'Complete Student Interactions',\n",
        "    'e-WBL Class Presentation': 'Complete Student Interactions',\n",
        "    'Job Fair': 'Complete Student Interactions',\n",
        "    'Class/Group Mentorship': 'Complete Student Interactions',\n",
        "    'Industry Sponsored Project': 'Complete Student Interactions',\n",
        "    'Class Presentation': 'Complete Student Interactions',\n",
        "    'Job Shadow': 'Complete Student Interactions',\n",
        "    'Site Visit': 'Complete Student Interactions',\n",
        "    'Speakers Bureau': 'Complete Student Interactions',\n",
        "    'Event': 'Complete Student Interactions',\n",
        "    'Individual Mentorship': 'Complete Student Interactions',\n",
        "    'Paid Job': 'Complete Student Interactions',\n",
        "    'Internship 60': 'Complete Student Internships',\n",
        "    'Internship 120': 'Complete Student Internships',\n",
        "    'Internship 320': 'Complete Student Internships',\n",
        "    'Apprenticeship': 'Complete Student Internships',\n",
        "}\n",
        "\n",
        "# WBL Opportunity Type to numeric column mapping (Pending)\n",
        "wbl_to_column_map_pending = {\n",
        "    'Professionalism 101 Training': 'Pending Student Interactions',\n",
        "    'Career Story Video': 'Pending Student Interactions',\n",
        "    'e-WBL Informational Interview': 'Pending Student Interactions',\n",
        "    'Industry Sponsored Project': 'Pending Student Interactions',\n",
        "    'Class Presentation': 'Pending Student Interactions',\n",
        "    'Job Shadow': 'Pending Student Interactions',\n",
        "    'Site Visit': 'Pending Student Interactions',\n",
        "    'Speakers Bureau': 'Pending Student Interactions',\n",
        "    'Event': 'Pending Student Interactions',\n",
        "    'Internship 60': 'Internships in Progress',\n",
        "    'Internship 120': 'Internships in Progress',\n",
        "    'Internship 320': 'Internships in Progress',\n",
        "    'Apprenticeship': 'Internships in Progress',\n",
        "}\n",
        "\n",
        "# Fill all-zero rows with minimum values\n",
        "for idx in dfmain[all_zeros].index:\n",
        "    row = dfmain.loc[idx]\n",
        "    wbl_type = row['WBL Opportunity Type']\n",
        "    category = row['Placement Status Category']\n",
        "\n",
        "    # Get minimum value for this WBL type (default to 1 if no minimum available)\n",
        "    min_value = wbl_minimums.get(wbl_type, 1)\n",
        "\n",
        "    if category == 'Completed':\n",
        "        if wbl_type in wbl_to_column_map_completed:\n",
        "            dfmain.loc[idx, wbl_to_column_map_completed[wbl_type]] = min_value\n",
        "    elif category == 'Pending':\n",
        "        if wbl_type in wbl_to_column_map_pending:\n",
        "            dfmain.loc[idx, wbl_to_column_map_pending[wbl_type]] = min_value\n",
        "    elif category == 'Declined':\n",
        "        dfmain.loc[idx, 'Declined Student Interactions'] = min_value\n",
        "    elif category == 'Cancelled':\n",
        "        dfmain.loc[idx, 'Cancelled Student Interactions'] = min_value\n",
        "\n",
        "# Drop temporary numeric sum column\n",
        "dfmain = dfmain.drop(columns=['_numeric_sum'])\n",
        "\n",
        "# Verify\n",
        "all_zeros_after = (dfmain[numeric_cols] == 0).all(axis=1)\n",
        "print(f\"\\nAll-zero rows before filling: {all_zeros.sum()}\")\n",
        "print(f\"All-zero rows after filling: {all_zeros_after.sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX3hbpXk4SHL",
        "outputId": "40dbf7fa-0d34-4370-d095-df2f79f48936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Logged 222 original rows: All-zero rows\n",
            "Minimum values by WBL Opportunity Type (for types with all-zero rows):\n",
            "  Apprenticeship: 1.0\n",
            "  Career Story Video: 1.0\n",
            "  Class Presentation: 1.0\n",
            "  Event: 1.0\n",
            "  Industry Sponsored Project: 1.0\n",
            "  Internship 60: 1.0\n",
            "  Job Shadow: 1.0\n",
            "  Professionalism 101 Training: 1.0\n",
            "  Regional Advisory Meeting: 1\n",
            "  Site Visit: 1.0\n",
            "  Site Visit - Staff: 1\n",
            "  Speakers Bureau: 10.0\n",
            "  Student Training: 1.0\n",
            "  e-WBL Class Presentation: 1.0\n",
            "  e-WBL Informational Interview: 1.0\n",
            "\n",
            "WBL Opportunity Type breakdown for all-zero rows:\n",
            "WBL Opportunity Type\n",
            "Speakers Bureau                  489\n",
            "Professionalism 101 Training     141\n",
            "Site Visit                       134\n",
            "Class Presentation                98\n",
            "e-WBL Class Presentation          91\n",
            "Internship 60                     62\n",
            "Regional Advisory Meeting         42\n",
            "Student Training                   9\n",
            "Job Shadow                         8\n",
            "Event                              5\n",
            "e-WBL Informational Interview      5\n",
            "Site Visit - Staff                 4\n",
            "Industry Sponsored Project         1\n",
            "Career Story Video                 1\n",
            "Apprenticeship                     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Total all-zero rows: 1091\n",
            "\n",
            "All-zero rows before filling: 1091\n",
            "All-zero rows after filling: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Duplicate Handling & Pending Events"
      ],
      "metadata": {
        "id": "ZPZYqUnN3Nga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Duplicate Detection (LOG: kept and dropped duplicates)"
      ],
      "metadata": {
        "id": "PVMoIwxBMIyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Log a single duplicate pair - the kept row AND its dropped duplicate. Both rows share the same _log_id.\n",
        "\n",
        "def log_duplicate_pair(kept_row, dropped_row, reason, description):\n",
        "\n",
        "    global changes_log, current_log_id\n",
        "\n",
        "    # Log kept row\n",
        "    kept_entry = kept_row.to_frame().T.copy()\n",
        "    kept_entry['_log_id'] = current_log_id\n",
        "    ...\n",
        "\n",
        "    # Log dropped row with SAME ID\n",
        "    dropped_entry = dropped_row.to_frame().T.copy()\n",
        "    dropped_entry['_log_id'] = current_log_id\n",
        "    ...\n",
        "\n",
        "    current_log_id += 1  # Increment AFTER both are assigned the same ID"
      ],
      "metadata": {
        "id": "L3SQgqq8G_i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base matching fields for duplicate detection\n",
        "base_match_fields = [\n",
        "    'Business Champion Name',\n",
        "    'Student Sponsor Name',\n",
        "    'WBL Opportunity Type',\n",
        "    'District',\n",
        "    'Event Title',\n",
        "    'School or Program Site'\n",
        "]\n",
        "\n",
        "# Create temporary column for numeric sum\n",
        "dfmain['_numeric_sum'] = dfmain[numeric_cols].sum(axis=1)\n",
        "\n",
        "# Find duplicates and decide which to keep\n",
        "rows_to_drop = []\n",
        "duplicate_pairs = []  # Store (kept_idx, dropped_idx) pairs\n",
        "\n",
        "for idx in dfmain.index:\n",
        "    if idx in rows_to_drop:\n",
        "        continue\n",
        "\n",
        "    row = dfmain.loc[idx]\n",
        "\n",
        "    # Look for matches in earlier rows\n",
        "    matches = dfmain[\n",
        "        (dfmain.index < idx) &\n",
        "        (~dfmain.index.isin(rows_to_drop)) &\n",
        "        (dfmain['Business Champion Name'] == row['Business Champion Name']) &\n",
        "        (dfmain['Student Sponsor Name'] == row['Student Sponsor Name']) &\n",
        "        (dfmain['WBL Opportunity Type'] == row['WBL Opportunity Type']) &\n",
        "        (dfmain['District'] == row['District']) &\n",
        "        (dfmain['Event Title'] == row['Event Title']) &\n",
        "        (dfmain['School or Program Site'] == row['School or Program Site']) &\n",
        "        (dfmain['_numeric_sum'] == row['_numeric_sum']) &\n",
        "        (\n",
        "            (dfmain['Initiation Date'] == row['Initiation Date']) |\n",
        "            (dfmain['Derived Event Date'] == row['Derived Event Date'])\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    if len(matches) > 0:\n",
        "        orig_idx = matches.index[0]\n",
        "        orig_status = dfmain.loc[orig_idx, 'Placement Status']\n",
        "        dup_status = row['Placement Status']\n",
        "\n",
        "        # Determine which to keep\n",
        "        keep_idx = None\n",
        "        drop_idx = None\n",
        "\n",
        "        # Rule 1: Prefer 'Completed' status\n",
        "        if orig_status == 'Completed' and dup_status != 'Completed':\n",
        "            keep_idx, drop_idx = orig_idx, idx\n",
        "        elif dup_status == 'Completed' and orig_status != 'Completed':\n",
        "            keep_idx, drop_idx = idx, orig_idx\n",
        "        else:\n",
        "            # Rule 2: Choose the one with least nulls\n",
        "            orig_null_count = dfmain.loc[orig_idx].isna().sum()\n",
        "            dup_null_count = row.isna().sum()\n",
        "\n",
        "            if orig_null_count < dup_null_count:\n",
        "                keep_idx, drop_idx = orig_idx, idx\n",
        "            elif dup_null_count < orig_null_count:\n",
        "                keep_idx, drop_idx = idx, orig_idx\n",
        "            else:\n",
        "                # Rule 3: Keep the most recent based on Derived Event Date\n",
        "                orig_date = dfmain.loc[orig_idx, 'Derived Event Date']\n",
        "                dup_date = row['Derived Event Date']\n",
        "\n",
        "                if pd.isna(orig_date) and pd.isna(dup_date):\n",
        "                    keep_idx, drop_idx = orig_idx, idx\n",
        "                elif pd.isna(orig_date):\n",
        "                    keep_idx, drop_idx = idx, orig_idx\n",
        "                elif pd.isna(dup_date):\n",
        "                    keep_idx, drop_idx = orig_idx, idx\n",
        "                elif dup_date > orig_date:\n",
        "                    keep_idx, drop_idx = idx, orig_idx\n",
        "                else:\n",
        "                    keep_idx, drop_idx = orig_idx, idx\n",
        "\n",
        "        rows_to_drop.append(drop_idx)\n",
        "        duplicate_pairs.append((keep_idx, drop_idx))\n",
        "\n",
        "# Remove duplicates from rows_to_drop list\n",
        "rows_to_drop = list(set(rows_to_drop))\n",
        "\n",
        "# LOG: each duplicate pair with the same _log_id\n",
        "if len(duplicate_pairs) > 0:\n",
        "    print(f\"  Logging {len(duplicate_pairs)} duplicate pairs...\")\n",
        "    for kept_idx, dropped_idx in duplicate_pairs:\n",
        "        kept_row = dfmain.loc[kept_idx]\n",
        "        dropped_row = dfmain.loc[dropped_idx]\n",
        "        log_duplicate_pair(kept_row, dropped_row, \"Duplicates\", \"identified using business name, student/sponsor, WBL type, district, event title, school, engagement count, initiation date/derived event date\")\n",
        "    print(f\"  Logged {len(duplicate_pairs)} duplicate pairs: Duplicates\")\n",
        "\n",
        "# Drop duplicates\n",
        "dfmain = dfmain.drop(rows_to_drop)\n",
        "\n",
        "# Drop temporary numeric sum column\n",
        "dfmain = dfmain.drop(columns=['_numeric_sum'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KCLK8XSiuAn",
        "outputId": "cb64a840-d402-4276-ef3c-721348c0f288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Logging 209 duplicate pairs...\n",
            "  Logged 209 duplicate pairs: Duplicates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fix Internship Rows (LOG: original and modified)"
      ],
      "metadata": {
        "id": "z0rqkuNwLwNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix 2019-2020 Internship rows marked as Declined that should be Completed\n",
        "declined_should_be_completed = dfmain[\n",
        "    (dfmain['Placement Status Category'] == 'Declined') &\n",
        "    (dfmain['School Year'] == '2019-2020') &\n",
        "    (dfmain['WBL Opportunity Type'].isin(['Internship 60', 'Internship 120', 'Internship 320'])) &\n",
        "    (dfmain['Complete Student Interactions'] > 0)\n",
        "].index\n",
        "\n",
        "if len(declined_should_be_completed) > 0:\n",
        "    # LOG: original rows only\n",
        "    original_rows = dfmain.loc[declined_should_be_completed].copy()\n",
        "    log_original_only(original_rows, \"2019-2020 completed Internship rows marked as 'Declined'\", \"marked as 'Completed'\")\n",
        "\n",
        "    # Make modifications\n",
        "    dfmain.loc[declined_should_be_completed, 'Placement Status'] = 'Completed'\n",
        "    dfmain.loc[declined_should_be_completed, 'Placement Status Category'] = 'Completed'\n",
        "    dfmain.loc[declined_should_be_completed, 'Complete Student Internships'] = dfmain.loc[declined_should_be_completed, 'Complete Student Interactions']\n",
        "    dfmain.loc[declined_should_be_completed, 'Complete Student Interactions'] = 0\n",
        "\n",
        "# Mark 'Internship In Process' 2024-2025 rows as 'Completed' (NOT LOGGED)\n",
        "internship_2024_25 = (\n",
        "    (dfmain['Placement Status'] == 'Internship In Process') &\n",
        "    (dfmain['School Year'] == '2024-2025')\n",
        ")\n",
        "dfmain.loc[internship_2024_25, 'Placement Status'] = 'Completed'\n",
        "dfmain.loc[internship_2024_25, 'Placement Status Category'] = 'Completed'\n",
        "\n",
        "# Transfer internship counts to Complete Student Internships for these rows\n",
        "dfmain.loc[internship_2024_25, 'Complete Student Internships'] = dfmain.loc[internship_2024_25, 'Internships in Progress']\n",
        "dfmain.loc[internship_2024_25, 'Internships in Progress'] = 0"
      ],
      "metadata": {
        "id": "ZGGEJ6lcLzjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reclassify Pending Statuses (LOG: original and modified)"
      ],
      "metadata": {
        "id": "_mk0q8IGL-jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reclassify remaining pending statuses as \"Declined - Unfinished\"\n",
        "pending_statuses = [\n",
        "    'Initial Contact Made', 'Pending-Scheduling', 'Scheduled Interview',\n",
        "    'Internship In Process', 'Scheduled Event (Pending Completion)'\n",
        "]\n",
        "\n",
        "pending_mask = dfmain['Placement Status'].isin(pending_statuses)\n",
        "\n",
        "if pending_mask.sum() > 0:\n",
        "    # LOG: original rows only\n",
        "    original_pending_rows = dfmain[pending_mask].copy()\n",
        "    log_original_only(original_pending_rows, \"Reclassify remaining pending statuses to 'Declined'\", \"marked as 'Declined - Unfinished'\")\n",
        "\n",
        "dfmain.loc[dfmain['Placement Status'].isin(pending_statuses), 'Placement Status'] = 'Declined - Unfinished'\n",
        "dfmain.loc[dfmain['Placement Status'] == 'Declined - Unfinished', 'Placement Status Category'] = 'Declined'\n",
        "\n",
        "# Transfer pending counts to declined counts for reclassified rows\n",
        "dfmain.loc[\n",
        "    dfmain['Placement Status'] == 'Declined - Unfinished',\n",
        "    'Declined Student Interactions'\n",
        "] = (\n",
        "    dfmain.loc[dfmain['Placement Status'] == 'Declined - Unfinished', 'Pending Student Interactions'] +\n",
        "    dfmain.loc[dfmain['Placement Status'] == 'Declined - Unfinished', 'Internships in Progress']\n",
        ")\n",
        "\n",
        "# Zero out the pending columns for these rows\n",
        "dfmain.loc[dfmain['Placement Status'] == 'Declined - Unfinished', 'Pending Student Interactions'] = 0\n",
        "dfmain.loc[dfmain['Placement Status'] == 'Declined - Unfinished', 'Internships in Progress'] = 0\n",
        "\n",
        "# Drop the now-irrelevant pending columns\n",
        "dfmain = dfmain.drop(columns=['Pending Student Interactions', 'Internships in Progress'])\n",
        "\n",
        "# Verify\n",
        "print(f\"Rows dropped as duplicates: {len(rows_to_drop)}\")\n",
        "print(f\"Final row count: {len(dfmain)}\")\n",
        "print(f\"\\nPlacement Status counts:\")\n",
        "print(dfmain['Placement Status'].value_counts())"
      ],
      "metadata": {
        "id": "bvVVlRwrMGzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829e304f-ef7a-4b53-f118-dfa64014b132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Logged 38 original rows: Reclassify remaining pending statuses to 'Declined'\n",
            "Rows dropped as duplicates: 209\n",
            "Final row count: 9918\n",
            "\n",
            "Placement Status counts:\n",
            "Placement Status\n",
            "Completed                           7644\n",
            "Declined-Applicant                   475\n",
            "Declined - Business Unresponsive     225\n",
            "Declined/Cancelled-Other             224\n",
            "Declined-Business Scheduling         220\n",
            "Declined-Business                    210\n",
            "Declined- Student Applicant          181\n",
            "Declined - Student Profile           176\n",
            "Declined - Student Other             113\n",
            "Declined - Student Unresponsive       90\n",
            "Declined - Staff Scheduling           76\n",
            "Declined-Intern NOT Selected          66\n",
            "Cancelled-COVID                       63\n",
            "Declined - Unfinished                 38\n",
            "Declined - Staff Applicant            35\n",
            "Declined - Staff Unresponsive         26\n",
            "Declined-Opportunity FULL             24\n",
            "Cancelled-Weather                     17\n",
            "Terminated                            14\n",
            "Cancelled-Illness                      1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardizing District Names and Reorganize Column Structure"
      ],
      "metadata": {
        "id": "suD1pQZpQGew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize District Names\n",
        "\n",
        "district_mapping = {\n",
        "    'D11': 'Colorado Springs (D11)',\n",
        "    'D20': 'Academy (D20)',\n",
        "    'D49': 'El Paso County (D49)',\n",
        "    'D2': 'Harrison (D2)',\n",
        "    'D3': 'Widefield (D3)',\n",
        "    'D8': 'Fountain-Fort Carson (D8)',\n",
        "    'D12': 'Cheyenne Mountain (D12)',\n",
        "    'D14': 'Manitou Springs (D14)',\n",
        "    'D38': 'Lewis-Palmer (D38)',\n",
        "    'CEC-CS': 'Colorado Springs Early College (CEC-CS)',\n",
        "    'BLR': 'Banning Lewis Ranch (BLR)',\n",
        "    'WPSD': 'Woodland Park (WPSD)',\n",
        "    'Ellicott': 'Ellicott (D22)',\n",
        "    'Peyton': 'Peyton (D23JT)',\n",
        "    'Calhan': 'Calhan (RJ-1)',\n",
        "    'Atlas Prep': 'Atlas Preparatory',\n",
        "    'CPA/PPOS': 'CO Digital BOCES (CPA/PPOS)',\n",
        "    'PTEC': 'Power Technical (PTEC)',\n",
        "    'Goal H.S.': 'Goal High School',\n",
        "    'Mon Impact': 'Monumental Impact',\n",
        "    'Peak Ed': 'Peak Education',\n",
        "    'Miami-Yoder': 'Miami-Yoder (JT-60)',\n",
        "    'ECA': 'Evangel Christian Academy',\n",
        "    'MET': 'Mountain Employment Training',\n",
        "    'TCA': 'The Classical Academy',\n",
        "    'CCV': 'Cripple Creek-Victor (RE-1)',\n",
        "    'DYS': 'Division of Youth Services',\n",
        "    'Vanguard': 'Vanguard School',\n",
        "    'Homeschool': 'Homeschool',\n",
        "    'Various': 'Various',\n",
        "    'BBBS': 'Big Brothers Big Sisters',\n",
        "    'Roundup': 'Roundup School'\n",
        "}\n",
        "\n",
        "dfmain['District'] = dfmain['District'].replace(district_mapping)\n",
        "\n",
        "# Reorganize Column Structure\n",
        "\n",
        "column_order = [\n",
        "    'School Year',\n",
        "    'Derived Event Date',\n",
        "    'Placement Status Category',\n",
        "    'Placement Status',\n",
        "    'WBL Opportunity Type',\n",
        "    'Event Title',\n",
        "    'Business Champion Name',\n",
        "    'Student Sponsor Name',\n",
        "    'District',\n",
        "    'School or Program Site',\n",
        "    'Complete Student Interactions',\n",
        "    'Complete Student Trainings',\n",
        "    'Complete Student Internships',\n",
        "    'Declined Student Interactions',\n",
        "    'Cancelled Student Interactions',\n",
        "    'Complete Staff Trainings',\n",
        "    'Initiation Date',\n",
        "    'Status Update Date',\n",
        "    'Event Date or Start Date',\n",
        "]\n",
        "\n",
        "dfmain = dfmain[column_order]"
      ],
      "metadata": {
        "id": "l3z7dB8YQJo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export"
      ],
      "metadata": {
        "id": "udJ3-mq4MSfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change Log Summary & Export"
      ],
      "metadata": {
        "id": "kBnVtmF9MUky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"CHANGES LOG SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nTotal rows logged: {len(changes_log):,}\")\n",
        "\n",
        "# print(\"\\nChanges by Type:\")\n",
        "# print(changes_log['_change_type'].value_counts().to_string())\n",
        "\n",
        "print(\"\\nChanges by Reason:\")\n",
        "print(changes_log['_change_reason_description'].value_counts().to_string())\n",
        "\n",
        "print(\"\\nChanges by Row Version:\")\n",
        "print(changes_log['_row_version'].value_counts().to_string())\n",
        "\n",
        "# Sort changes log for easier review (group by log_id so pairs are together)\n",
        "changes_log_sorted = changes_log.sort_values(['_log_id', '_row_version'])\n",
        "\n",
        "# Reorder columns for export\n",
        "log_column_order = [\n",
        "    '_log_id',\n",
        "    '_change_reason_description',\n",
        "    # '_change_type',\n",
        "    '_row_version',\n",
        "    'School Year',\n",
        "    'Derived Event Date',\n",
        "    'Placement Status Category',\n",
        "    'Placement Status',\n",
        "    'WBL Opportunity Type',\n",
        "    'Event Title',\n",
        "    'Business Champion Name',\n",
        "    'Student Sponsor Name',\n",
        "    'District',\n",
        "    'School or Program Site',\n",
        "    'Complete Student Interactions',\n",
        "    'Complete Student Trainings',\n",
        "    'Complete Student Internships',\n",
        "    'Declined Student Interactions',\n",
        "    'Cancelled Student Interactions',\n",
        "    'Complete Staff Trainings',\n",
        "    'Initiation Date',\n",
        "    'Status Update Date',\n",
        "    'Event Date or Start Date',\n",
        "    'PPBEA Notes'\n",
        "]\n",
        "\n",
        "# Only include columns that exist in the log\n",
        "existing_cols = [col for col in log_column_order if col in changes_log_sorted.columns]\n",
        "changes_log_export = changes_log_sorted[existing_cols]\n",
        "\n",
        "# Export changes log\n",
        "import os\n",
        "changes_output_path = '/content/drive/MyDrive/Work/BEA/2025 BEA Data Project Shared Folder/Data/(Main) Data Sources/Existing/PPBEA Pipeline/Cleaned/'\n",
        "os.makedirs(changes_output_path, exist_ok=True)\n",
        "\n",
        "changes_file = f'{changes_output_path}PPBEA_Pipeline_Changes_Log.csv'\n",
        "changes_log_export.to_csv(changes_file, index=False)\n",
        "\n",
        "print(f\"\\nChanges log exported to: {changes_file}\")\n",
        "print(f\"Total rows in log: {len(changes_log):,}\")"
      ],
      "metadata": {
        "id": "NkrowugZMWWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734393e0-1be6-492f-9614-a8adf298d697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CHANGES LOG SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Total rows logged: 514\n",
            "\n",
            "Changes by Reason:\n",
            "_change_reason_description\n",
            "All-zero rows - filled with min value based on WBL type                                            222\n",
            "Separate Pro101 Cert from non-Pro101 event - split into original event and new Pro101 row          122\n",
            "Summary/empty rows - dropped                                                                        96\n",
            "Reclassify remaining pending statuses to 'Declined' - marked as 'Declined - Unfinished'             38\n",
            "Fix student trainings and interactions double-counting - zeroed 'Complete Student Interactions'     24\n",
            "Unrealistic dates - set to null in Status Update Date                                                7\n",
            "Unrealistic dates - set to null in Initiation Date                                                   4\n",
            "One-off WBL type - dropped                                                                           1\n",
            "\n",
            "Changes by Row Version:\n",
            "_row_version\n",
            "original    356\n",
            "dropped      97\n",
            "created      61\n",
            "\n",
            "Changes log exported to: /content/drive/MyDrive/Work/BEA/2025 BEA Data Project Shared Folder/Data/(Main) Data Sources/Existing/PPBEA Pipeline/Cleaned/PPBEA_Pipeline_Changes_Log.csv\n",
            "Total rows in log: 514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Data Summary"
      ],
      "metadata": {
        "id": "DdY40u6EPixE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"FINAL DATASET SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Size\n",
        "print(f\"\\nSize: {dfmain.shape[0]:,} rows × {dfmain.shape[1]} columns\")\n",
        "\n",
        "# Date Range\n",
        "print(f\"\\nDate Range: {dfmain['Derived Event Date'].min().strftime('%B %d, %Y')} - {dfmain['Derived Event Date'].max().strftime('%B %d, %Y')}\")\n",
        "\n",
        "# Records by Year\n",
        "print(\"\\nRecords by Year:\")\n",
        "year_counts = dfmain['School Year'].value_counts().sort_index()\n",
        "for year, count in year_counts.items():\n",
        "    print(f\"  {year}: {count:,}\")\n",
        "\n",
        "# Status Distribution (by Category)\n",
        "print(\"\\nStatus Distribution (by Category):\")\n",
        "status_counts = dfmain['Placement Status Category'].value_counts()\n",
        "for status, count in status_counts.items():\n",
        "    pct = count / len(dfmain) * 100\n",
        "    print(f\"  {status}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# Status Distribution (detailed)\n",
        "print(\"\\nStatus Distribution (detailed):\")\n",
        "detailed_status = dfmain['Placement Status'].value_counts()\n",
        "for status, count in detailed_status.items():\n",
        "    pct = count / len(dfmain) * 100\n",
        "    print(f\"  {status}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# Student Engagement Metrics\n",
        "print(\"\\nStudent Engagement Metrics:\")\n",
        "total_completed = dfmain['Complete Student Interactions'].sum() + dfmain['Complete Student Trainings'].sum() + dfmain['Complete Student Internships'].sum()\n",
        "total_declined = dfmain['Declined Student Interactions'].sum()\n",
        "total_cancelled = dfmain['Cancelled Student Interactions'].sum()\n",
        "grand_total = total_completed + total_declined + total_cancelled\n",
        "\n",
        "print(f\"  Total Completed: {total_completed:,.0f}\")\n",
        "print(f\"  Total Declined: {total_declined:,.0f}\")\n",
        "print(f\"  Total Cancelled: {total_cancelled:,.0f}\")\n",
        "print(f\"  Grand Total: {grand_total:,.0f} student engagements\")\n",
        "\n",
        "# Average students per completed event\n",
        "completed_events = dfmain[dfmain['Placement Status Category'] == 'Completed']\n",
        "avg_students = total_completed / len(completed_events) if len(completed_events) > 0 else 0\n",
        "print(f\"  Average Students per Completed Event: {avg_students:.1f}\")\n",
        "\n",
        "# Staff Metrics\n",
        "print(\"\\nStaff Metrics:\")\n",
        "print(f\"  Staff Trainings: {dfmain['Complete Staff Trainings'].sum():,.0f}\")\n",
        "\n",
        "# Top 5 Districts\n",
        "print(\"\\nTop 5 Districts:\")\n",
        "top_districts = dfmain['District'].value_counts().head(5)\n",
        "for i, (district, count) in enumerate(top_districts.items(), 1):\n",
        "    print(f\"  {i}. {district}: {count:,} events\")\n",
        "\n",
        "# Top 5 WBL Types\n",
        "print(\"\\nTop 5 WBL Opportunity Types:\")\n",
        "top_wbl = dfmain['WBL Opportunity Type'].value_counts().head(5)\n",
        "for i, (wbl, count) in enumerate(top_wbl.items(), 1):\n",
        "    print(f\"  {i}. {wbl}: {count:,} events\")\n",
        "\n",
        "# Top 5 Businesses\n",
        "print(\"\\nTop 5 Business Partners:\")\n",
        "top_business = dfmain['Business Champion Name'].value_counts().head(5)\n",
        "for i, (business, count) in enumerate(top_business.items(), 1):\n",
        "    print(f\"  {i}. {business}: {count:,} events\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lg_wC-ePig4",
        "outputId": "aeebeb58-ca4c-471b-aac8-307ad84889b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "FINAL DATASET SUMMARY\n",
            "============================================================\n",
            "\n",
            "Size: 9,918 rows × 19 columns\n",
            "\n",
            "Date Range: January 07, 2019 - September 22, 2025\n",
            "\n",
            "Records by Year:\n",
            "  2019-2020: 309\n",
            "  2020-2021: 1,926\n",
            "  2021-2022: 1,993\n",
            "  2022-2023: 2,743\n",
            "  2023-2024: 1,502\n",
            "  2024-2025: 1,445\n",
            "\n",
            "Status Distribution (by Category):\n",
            "  Completed: 7,644 (77.1%)\n",
            "  Declined: 2,193 (22.1%)\n",
            "  Cancelled: 81 (0.8%)\n",
            "\n",
            "Status Distribution (detailed):\n",
            "  Completed: 7,644 (77.1%)\n",
            "  Declined-Applicant: 475 (4.8%)\n",
            "  Declined - Business Unresponsive: 225 (2.3%)\n",
            "  Declined/Cancelled-Other: 224 (2.3%)\n",
            "  Declined-Business Scheduling: 220 (2.2%)\n",
            "  Declined-Business: 210 (2.1%)\n",
            "  Declined- Student Applicant: 181 (1.8%)\n",
            "  Declined - Student Profile: 176 (1.8%)\n",
            "  Declined - Student Other: 113 (1.1%)\n",
            "  Declined - Student Unresponsive: 90 (0.9%)\n",
            "  Declined - Staff Scheduling: 76 (0.8%)\n",
            "  Declined-Intern NOT Selected: 66 (0.7%)\n",
            "  Cancelled-COVID: 63 (0.6%)\n",
            "  Declined - Unfinished: 38 (0.4%)\n",
            "  Declined - Staff Applicant: 35 (0.4%)\n",
            "  Declined - Staff Unresponsive: 26 (0.3%)\n",
            "  Declined-Opportunity FULL: 24 (0.2%)\n",
            "  Cancelled-Weather: 17 (0.2%)\n",
            "  Terminated: 14 (0.1%)\n",
            "  Cancelled-Illness: 1 (0.0%)\n",
            "\n",
            "Student Engagement Metrics:\n",
            "  Total Completed: 50,856\n",
            "  Total Declined: 8,742\n",
            "  Total Cancelled: 1,764\n",
            "  Grand Total: 61,362 student engagements\n",
            "  Average Students per Completed Event: 6.7\n",
            "\n",
            "Staff Metrics:\n",
            "  Staff Trainings: 1,382\n",
            "\n",
            "Top 5 Districts:\n",
            "  1. Colorado Springs (D11): 3,246 events\n",
            "  2. Academy (D20): 1,316 events\n",
            "  3. El Paso County (D49): 1,302 events\n",
            "  4. Fountain-Fort Carson (D8): 755 events\n",
            "  5. Widefield (D3): 711 events\n",
            "\n",
            "Top 5 WBL Opportunity Types:\n",
            "  1. Professionalism 101 Training: 3,807 events\n",
            "  2. e-WBL Informational Interview: 1,498 events\n",
            "  3. Job Shadow: 1,167 events\n",
            "  4. Internship 60: 964 events\n",
            "  5. Speakers Bureau: 752 events\n",
            "\n",
            "Top 5 Business Partners:\n",
            "  1. PPBEA: 4,213 events\n",
            "  2. UCCS College of Nursing & Health Sciences: 308 events\n",
            "  3. Children’s Hospital Colorado, Colorado Springs: 220 events\n",
            "  4. Jaxon Engineering & Maintenance: 180 events\n",
            "  5. D49 - El Paso County School District 49: 125 events\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reconciliation Summary"
      ],
      "metadata": {
        "id": "ENLd0rVQMhhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"RECONCILIATION SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "company_reported = {\n",
        "    '2019-2020': 3233,\n",
        "    '2020-2021': 4056,\n",
        "    '2021-2022': 6787,\n",
        "    '2022-2023': 9815,\n",
        "    '2023-2024': 11865,\n",
        "    '2024-2025': 14135\n",
        "}\n",
        "\n",
        "# Calculate final numbers\n",
        "complete_cols_final = ['Complete Student Interactions', 'Complete Student Trainings', 'Complete Student Internships']\n",
        "\n",
        "print(f\"\\n{'Year':<15} {'Company':<12} {'Our Final':<12} {'Difference':<12} {'% Diff':<10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "total_company = 0\n",
        "total_ours = 0\n",
        "\n",
        "for year in sorted(dfmain['School Year'].unique()):\n",
        "    year_df = dfmain[(dfmain['School Year'] == year) & (dfmain['Placement Status Category'] == 'Completed')]\n",
        "    our_complete = year_df[complete_cols_final].sum().sum()\n",
        "\n",
        "    company_num = company_reported.get(year, 0)\n",
        "    diff = our_complete - company_num\n",
        "    pct_diff = (diff / company_num * 100) if company_num > 0 else 0\n",
        "\n",
        "    total_company += company_num\n",
        "    total_ours += our_complete\n",
        "\n",
        "    print(f\"{year:<15} {company_num:<12,} {our_complete:<12,.0f} {diff:<+12,.0f} {pct_diff:<+10.1f}%\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "total_pct = (total_ours - total_company) / total_company * 100\n",
        "print(f\"{'TOTAL':<15} {total_company:<12,} {total_ours:<12,.0f} {total_ours - total_company:<+12,.0f} {total_pct:<+10.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ],
      "metadata": {
        "id": "9oUFhqSjMlKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42542e3d-ea19-48d5-d763-0fbf91fcc1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "RECONCILIATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Year            Company      Our Final    Difference   % Diff    \n",
            "----------------------------------------------------------------------\n",
            "2019-2020       3,233        3,201        -32          -1.0      %\n",
            "2020-2021       4,056        3,983        -73          -1.8      %\n",
            "2021-2022       6,787        6,955        +168         +2.5      %\n",
            "2022-2023       9,815        9,936        +121         +1.2      %\n",
            "2023-2024       11,865       11,980       +115         +1.0      %\n",
            "2024-2025       14,135       14,779       +644         +4.6      %\n",
            "----------------------------------------------------------------------\n",
            "TOTAL           49,891       50,834       +943         +1.9      %\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}